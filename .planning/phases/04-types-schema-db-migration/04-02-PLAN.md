---
phase: 04-types-schema-db-migration
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - supabase/migrations/20260216000000_v2_schema_expansion.sql
  - src/lib/engine/normalize.ts
autonomous: true

must_haves:
  truths:
    - "DB migration adds 7 new columns to analysis_results (behavioral_predictions, feature_vector, reasoning, warnings, input_mode, has_video, gemini_score)"
    - "DB migration adds 2 new columns to rule_library (evaluation_tier, rule_contributions)"
    - "DB migration adds 3 new indexes for v2 query patterns"
    - "Input normalization converts AnalysisInput to ContentPayload with hashtag extraction and duration hints"
    - "Migration SQL is idempotent (uses IF NOT EXISTS / ADD COLUMN IF NOT EXISTS)"
  artifacts:
    - path: "supabase/migrations/20260216000000_v2_schema_expansion.sql"
      provides: "v2 schema expansion migration"
      contains: "ALTER TABLE analysis_results"
    - path: "src/lib/engine/normalize.ts"
      provides: "Input normalization function"
      exports: ["normalizeInput"]
  key_links:
    - from: "src/lib/engine/normalize.ts"
      to: "src/lib/engine/types.ts"
      via: "imports AnalysisInput, ContentPayload types"
      pattern: "import.*AnalysisInput.*ContentPayload"
    - from: "src/lib/engine/normalize.ts"
      to: "src/lib/engine/pipeline.ts"
      via: "normalizeInput consumed by pipeline entry point (Phase 5 wiring)"
      pattern: "normalizeInput"
    - from: "supabase/migrations/20260216000000_v2_schema_expansion.sql"
      to: "supabase/migrations/20260213000000_content_intelligence.sql"
      via: "ALTERs tables created in base migration"
      pattern: "ALTER TABLE"
---

<objective>
Create the database migration for v2 schema expansion (new columns on analysis_results and rule_library, new indexes) and implement input normalization that converts the expanded AnalysisInput into a ContentPayload for pipeline consumption.

Purpose: The DB must store v2 prediction outputs (behavioral predictions, feature vectors, warnings). The normalizer bridges the gap between user-facing input (3 modes) and the internal pipeline representation.

Output: Migration SQL file ready to run, `normalize.ts` module exporting `normalizeInput()`.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-types-schema-db-migration/04-01-SUMMARY.md
@src/lib/engine/types.ts
@supabase/migrations/20260213000000_content_intelligence.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create v2 schema expansion migration</name>
  <files>supabase/migrations/20260216000000_v2_schema_expansion.sql</files>
  <action>
Create `supabase/migrations/20260216000000_v2_schema_expansion.sql` with these changes:

**analysis_results — 7 new columns:**
```sql
-- v2 Prediction Engine schema expansion
-- Adds behavioral predictions, feature vector, reasoning, and input mode tracking

-- New columns for v2 prediction outputs
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS behavioral_predictions JSONB;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS feature_vector JSONB;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS reasoning TEXT;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS warnings TEXT[] DEFAULT '{}';
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS input_mode TEXT DEFAULT 'text' CHECK (input_mode IN ('text', 'tiktok_url', 'video_upload'));
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS has_video BOOLEAN DEFAULT FALSE;
ALTER TABLE analysis_results ADD COLUMN IF NOT EXISTS gemini_score NUMERIC(5,2);
```

**rule_library — 2 new columns:**
```sql
-- Rule evaluation tier for hybrid rules (Phase 9)
ALTER TABLE rule_library ADD COLUMN IF NOT EXISTS evaluation_tier TEXT DEFAULT 'regex' CHECK (evaluation_tier IN ('regex', 'semantic'));
-- Per-rule accuracy tracking (Phase 9)
ALTER TABLE rule_library ADD COLUMN IF NOT EXISTS rule_contributions JSONB DEFAULT '{}';
```

**3 new indexes:**
```sql
-- Index for filtering by input mode
CREATE INDEX IF NOT EXISTS idx_analysis_results_input_mode ON analysis_results(input_mode) WHERE deleted_at IS NULL;
-- Index for video analyses
CREATE INDEX IF NOT EXISTS idx_analysis_results_has_video ON analysis_results(has_video) WHERE has_video = true AND deleted_at IS NULL;
-- Index for rule evaluation tier
CREATE INDEX IF NOT EXISTS idx_rule_library_evaluation_tier ON rule_library(evaluation_tier) WHERE is_active = true;
```

**Update content_type CHECK constraint** to add 'tiktok' as a valid type:
```sql
-- Expand content_type to include 'tiktok' for TikTok-specific content
ALTER TABLE analysis_results DROP CONSTRAINT IF EXISTS analysis_results_content_type_check;
ALTER TABLE analysis_results ADD CONSTRAINT analysis_results_content_type_check
  CHECK (content_type IN ('post', 'reel', 'story', 'video', 'thread', 'tiktok'));
```

Use `IF NOT EXISTS` / `ADD COLUMN IF NOT EXISTS` throughout for idempotency. Add a header comment explaining the migration purpose.
  </action>
  <verify>
Verify SQL syntax: `cat supabase/migrations/20260216000000_v2_schema_expansion.sql | grep -c "ALTER TABLE"` — should be 11+ ALTER statements.
Verify idempotency markers: `grep -c "IF NOT EXISTS" supabase/migrations/20260216000000_v2_schema_expansion.sql` — should match index and column additions.
  </verify>
  <done>Migration file exists at expected path with 7 analysis_results columns, 2 rule_library columns, 3 indexes, and expanded content_type CHECK. All statements use IF NOT EXISTS for safe re-runs.</done>
</task>

<task type="auto">
  <name>Task 2: Implement input normalization (AnalysisInput to ContentPayload)</name>
  <files>src/lib/engine/normalize.ts</files>
  <action>
Create `src/lib/engine/normalize.ts` that converts the expanded AnalysisInput into a ContentPayload ready for pipeline consumption.

```typescript
import type { AnalysisInput, ContentPayload } from "./types";

/**
 * Extract hashtags from text content.
 * Matches #word patterns, returns lowercase unique list.
 */
function extractHashtags(text: string): string[] {
  const matches = text.match(/#[\w\u00C0-\u024F]+/g);
  if (!matches) return [];
  return [...new Set(matches.map(h => h.toLowerCase()))];
}

/**
 * Estimate video duration from content text hints.
 * Looks for patterns like "30s", "1 min", "60 seconds" in the text.
 * Returns null if no hint found.
 */
function extractDurationHint(text: string): number | null {
  // Match "Xs" or "X seconds"
  const secMatch = text.match(/(\d+)\s*(?:s|sec|seconds?)\b/i);
  if (secMatch) return parseInt(secMatch[1]!, 10);

  // Match "X min" or "X minutes"
  const minMatch = text.match(/(\d+)\s*(?:min|minutes?)\b/i);
  if (minMatch) return parseInt(minMatch[1]!, 10) * 60;

  return null;
}

/**
 * Normalize AnalysisInput into ContentPayload for pipeline consumption.
 *
 * For text mode: uses content_text directly.
 * For tiktok_url mode: content_text is caption/script (optional), video_url set to TikTok URL.
 *   Actual video extraction happens in pipeline via Apify (Phase 5).
 * For video_upload mode: content_text is caption (optional), video_url points to Supabase Storage.
 *   Actual storage URL resolution happens in pipeline (Phase 5).
 */
export function normalizeInput(input: AnalysisInput): ContentPayload {
  const contentText = input.content_text ?? "";

  return {
    content_text: contentText,
    content_type: input.content_type,
    input_mode: input.input_mode,
    video_url: input.tiktok_url ?? input.video_storage_path ?? null,
    hashtags: extractHashtags(contentText),
    duration_hint: extractDurationHint(contentText),
    niche: input.niche ?? null,
    creator_handle: input.creator_handle ?? null,
    society_id: input.society_id ?? null,
  };
}
```

Key design decisions:
- Hashtag extraction uses Unicode-aware regex to support international hashtags
- Duration hint extraction is best-effort from text content (not authoritative — Apify/video metadata will override in Phase 5)
- `video_url` is set to TikTok URL or storage path as-is. Actual URL resolution (Apify extraction for TikTok, signed URL for Storage) is deferred to Phase 5 pipeline stages
- `content_text` defaults to empty string for URL/video modes where user didn't provide caption — the pipeline will populate from extracted metadata in Phase 5
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/engine/normalize.ts 2>&1 | head -10` — expect zero errors (depends on types from 04-01 being complete).
Verify exports: `grep "export function normalizeInput" src/lib/engine/normalize.ts` — exists.
Verify hashtag extraction logic: `grep "extractHashtags" src/lib/engine/normalize.ts` — exists.
  </verify>
  <done>normalizeInput() exported from normalize.ts. Converts AnalysisInput to ContentPayload with hashtag extraction, duration hints, and video URL passthrough. Ready for Phase 5 pipeline to consume.</done>
</task>

</tasks>

<verification>
1. Migration file exists: `ls supabase/migrations/20260216000000_v2_schema_expansion.sql`
2. Migration has all columns: `grep -c "ADD COLUMN" supabase/migrations/20260216000000_v2_schema_expansion.sql` >= 9
3. Migration has indexes: `grep -c "CREATE INDEX" supabase/migrations/20260216000000_v2_schema_expansion.sql` >= 3
4. normalize.ts exists: `ls src/lib/engine/normalize.ts`
5. normalize.ts exports normalizeInput: `grep "export function normalizeInput" src/lib/engine/normalize.ts`
6. normalize.ts imports from types.ts: `grep "import.*types" src/lib/engine/normalize.ts`
7. `npx tsc --noEmit src/lib/engine/normalize.ts` — zero errors
</verification>

<success_criteria>
- Migration SQL file adds 7 columns to analysis_results, 2 columns to rule_library, 3 new indexes
- All ALTER/CREATE statements use IF NOT EXISTS for idempotent re-runs
- normalizeInput() converts 3 input modes to ContentPayload
- Hashtag extraction works on content_text
- Duration hint extraction is best-effort from text
- video_url is passthrough (actual resolution deferred to Phase 5)
- TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/04-types-schema-db-migration/04-02-SUMMARY.md`
</output>
