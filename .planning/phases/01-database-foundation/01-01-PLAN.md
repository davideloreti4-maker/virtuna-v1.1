---
phase: 01-database-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/20260213000000_content_intelligence.sql
  - src/types/database.types.ts
autonomous: true

must_haves:
  truths:
    - "Running `supabase db push` succeeds and creates all 6 tables (scraped_videos, trending_sounds, analysis_results, outcomes, rule_library, usage_tracking)"
    - "Every table has RLS enabled with appropriate policies: public read for scraped_videos (non-archived), trending_sounds, and active rules; user-scoped for analysis_results, outcomes, and usage_tracking"
    - "All RLS policies use the `(SELECT auth.uid())` performance pattern"
    - "`supabase gen types typescript` produces a `database.types.ts` that compiles with zero TypeScript errors"
    - "Every table has `created_at` and `updated_at` columns with an `update_updated_at_column()` trigger"
    - "scraped_videos has `archived_at` soft delete; analysis_results and outcomes have `deleted_at` soft delete"
  artifacts:
    - path: "supabase/migrations/20260213000000_content_intelligence.sql"
      provides: "5 core tables + usage_tracking + RLS policies + indexes + triggers"
      contains: "CREATE TABLE scraped_videos"
    - path: "src/types/database.types.ts"
      provides: "Auto-generated TypeScript types for all tables"
      contains: "scraped_videos"
  key_links:
    - from: "supabase/migrations/20260213000000_content_intelligence.sql"
      to: "src/types/database.types.ts"
      via: "supabase gen types typescript --local"
      pattern: "scraped_videos|trending_sounds|analysis_results|outcomes|rule_library|usage_tracking"
---

<objective>
Create the complete database schema migration for Virtuna's content intelligence platform: 5 core tables (scraped_videos, trending_sounds, analysis_results, outcomes, rule_library), 1 usage tracking table, RLS policies, performance indexes, soft delete columns, and updated_at triggers. Regenerate TypeScript types from the new schema.

Purpose: Every subsequent phase (engine, pipeline, API routes, client) depends on these tables and types existing. This is the foundation that unblocks all other work.

Output: One migration file with all tables, policies, indexes, and triggers. Regenerated `database.types.ts` with zero compilation errors.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-database-foundation/01-RESEARCH.md
@supabase/migrations/20260212000000_add_whop_subscriptions.sql
@src/types/database.types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create content intelligence migration with all tables, RLS, indexes, and triggers</name>
  <files>supabase/migrations/20260213000000_content_intelligence.sql</files>
  <action>
Create a new Supabase migration file at `supabase/migrations/20260213000000_content_intelligence.sql`. This single migration creates everything the content intelligence platform needs.

**1. Shared trigger function** (create first, referenced by all tables):

```sql
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

**2. scraped_videos table** (Apify pipeline output):
- Columns: `id` UUID PK (gen_random_uuid), `platform` TEXT NOT NULL DEFAULT 'tiktok', `platform_video_id` TEXT NOT NULL, `video_url` TEXT, `author` TEXT, `author_url` TEXT, `description` TEXT, `views` BIGINT DEFAULT 0, `likes` BIGINT DEFAULT 0, `shares` BIGINT DEFAULT 0, `comments` BIGINT DEFAULT 0, `sound_name` TEXT, `sound_url` TEXT, `hashtags` TEXT[] DEFAULT '{}', `category` TEXT, `duration_seconds` INTEGER, `metadata` JSONB DEFAULT '{}', `archived_at` TIMESTAMPTZ (soft delete per CONTEXT.md), `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- UNIQUE constraint on `(platform, platform_video_id)` to prevent duplicate scrapes
- RLS enabled. SELECT policy: `USING (archived_at IS NULL)` — public read for non-archived videos (defense-in-depth per RESEARCH.md). No INSERT/UPDATE/DELETE policies for anon/authenticated — writes happen via service role only (cron routes).
- Index: `CREATE INDEX idx_scraped_videos_category ON scraped_videos(category);`
- Index: `CREATE INDEX idx_scraped_videos_platform ON scraped_videos(platform);`
- Index: `CREATE INDEX idx_scraped_videos_created_at ON scraped_videos(created_at DESC);`
- Partial index: `CREATE INDEX idx_scraped_videos_non_archived ON scraped_videos(created_at DESC) WHERE archived_at IS NULL;`
- updated_at trigger

**3. trending_sounds table** (aggregated from scraped_videos):
- Columns: `id` UUID PK, `sound_name` TEXT NOT NULL, `sound_url` TEXT, `video_count` INTEGER DEFAULT 0, `total_views` BIGINT DEFAULT 0, `growth_rate` NUMERIC(10,4) DEFAULT 0, `velocity_score` NUMERIC(10,4) DEFAULT 0, `trend_phase` TEXT CHECK (trend_phase IN ('emerging', 'rising', 'peak', 'declining')), `first_seen` TIMESTAMPTZ DEFAULT NOW(), `last_seen` TIMESTAMPTZ DEFAULT NOW(), `metadata` JSONB DEFAULT '{}', `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- UNIQUE constraint on `sound_name` (one entry per sound)
- RLS enabled. SELECT policy: `USING (true)` — public read for all trending sounds. No write policies for anon/authenticated — service role only.
- Index: `CREATE INDEX idx_trending_sounds_velocity ON trending_sounds(velocity_score DESC);`
- Index: `CREATE INDEX idx_trending_sounds_trend_phase ON trending_sounds(trend_phase);`
- updated_at trigger

**4. analysis_results table** (user analysis output):
- Columns: `id` UUID PK, `user_id` UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE, `content_text` TEXT NOT NULL, `content_type` TEXT NOT NULL CHECK (content_type IN ('post', 'reel', 'story', 'video', 'thread')), `society_id` TEXT, `overall_score` NUMERIC(5,2), `confidence` NUMERIC(5,4), `factors` JSONB DEFAULT '[]', `suggestions` JSONB DEFAULT '[]', `personas` JSONB DEFAULT '[]', `variants` JSONB DEFAULT '[]', `insights` TEXT, `conversation_themes` JSONB DEFAULT '[]', `gemini_model` TEXT, `deepseek_model` TEXT, `engine_version` TEXT, `latency_ms` INTEGER, `cost_cents` NUMERIC(10,4), `rule_score` NUMERIC(5,2), `trend_score` NUMERIC(5,2), `ml_score` NUMERIC(5,2), `score_weights` JSONB DEFAULT '{}', `deleted_at` TIMESTAMPTZ (soft delete per CONTEXT.md), `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- RLS enabled. Policies use `(SELECT auth.uid())` wrapper:
  - SELECT: `USING (user_id = (SELECT auth.uid()) AND deleted_at IS NULL)` — user sees own non-deleted results
  - INSERT: `WITH CHECK (user_id = (SELECT auth.uid()))` — user can only insert for themselves
  - UPDATE: `USING (user_id = (SELECT auth.uid())) WITH CHECK (user_id = (SELECT auth.uid()))` — user can soft-delete own results (set deleted_at)
- Index: `CREATE INDEX idx_analysis_results_user_id ON analysis_results(user_id);`
- Index: `CREATE INDEX idx_analysis_results_created_at ON analysis_results(user_id, created_at DESC);`
- Partial index: `CREATE INDEX idx_analysis_results_non_deleted ON analysis_results(user_id, created_at DESC) WHERE deleted_at IS NULL;`
- updated_at trigger

**5. outcomes table** (actual performance vs predicted):
- Columns: `id` UUID PK, `analysis_id` UUID NOT NULL REFERENCES analysis_results(id) ON DELETE CASCADE, `user_id` UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE, `actual_views` BIGINT, `actual_likes` BIGINT, `actual_shares` BIGINT, `actual_engagement_rate` NUMERIC(10,6), `predicted_score` NUMERIC(5,2), `actual_score` NUMERIC(5,2), `delta` NUMERIC(5,2), `platform` TEXT, `platform_post_url` TEXT, `deleted_at` TIMESTAMPTZ (soft delete per CONTEXT.md), `reported_at` TIMESTAMPTZ DEFAULT NOW(), `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- UNIQUE constraint on `analysis_id` (one outcome per analysis)
- RLS enabled. Policies use `(SELECT auth.uid())` wrapper:
  - SELECT: `USING (user_id = (SELECT auth.uid()) AND deleted_at IS NULL)`
  - INSERT: `WITH CHECK (user_id = (SELECT auth.uid()))`
  - UPDATE: `USING (user_id = (SELECT auth.uid())) WITH CHECK (user_id = (SELECT auth.uid()))`
- Index: `CREATE INDEX idx_outcomes_user_id ON outcomes(user_id);`
- Index: `CREATE INDEX idx_outcomes_analysis_id ON outcomes(analysis_id);`
- updated_at trigger

**6. rule_library table** (merged schema per CONTEXT.md and RESEARCH.md):
- Columns: `id` UUID PK, `name` TEXT NOT NULL, `description` TEXT, `category` TEXT NOT NULL CHECK (category IN ('hook', 'retention', 'platform', 'audio', 'text', 'timing', 'creator')), `pattern` TEXT (architecture ref: deterministic pattern matching), `score_modifier` INTEGER (architecture ref: additive scoring), `platform` TEXT (NULL = cross-platform, "tiktok"/"instagram"/"youtube" = specific), `evaluation_prompt` TEXT (REQUIREMENTS.md: LLM evaluation), `weight` NUMERIC(5,3) NOT NULL DEFAULT 1.0, `max_score` NUMERIC(5,2) NOT NULL DEFAULT 10.0, `accuracy_rate` NUMERIC(5,4), `sample_count` INTEGER DEFAULT 0, `is_active` BOOLEAN DEFAULT TRUE, `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- RLS enabled. SELECT policy: `USING (is_active = true)` — public read for active rules only. No write policies for anon/authenticated — service role manages rules.
- Index: `CREATE INDEX idx_rule_library_category ON rule_library(category);`
- Index: `CREATE INDEX idx_rule_library_active ON rule_library(is_active) WHERE is_active = true;`
- updated_at trigger

**7. usage_tracking table** (tier-gated analysis limits):
- Columns: `id` UUID PK, `user_id` UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE, `period_start` DATE NOT NULL, `period_type` TEXT NOT NULL CHECK (period_type IN ('daily', 'monthly')), `analysis_count` INTEGER NOT NULL DEFAULT 0, `created_at` TIMESTAMPTZ DEFAULT NOW(), `updated_at` TIMESTAMPTZ DEFAULT NOW()
- UNIQUE constraint on `(user_id, period_start, period_type)` for atomic `INSERT ... ON CONFLICT DO UPDATE SET analysis_count = analysis_count + 1`
- RLS enabled. SELECT policy: `USING (user_id = (SELECT auth.uid()))` — user reads own usage. No write policies for anon/authenticated — service role increments usage.
- Index: `CREATE INDEX idx_usage_tracking_user_period ON usage_tracking(user_id, period_start, period_type);`
- updated_at trigger

**8. Apply updated_at triggers to existing tables that lack them:**
- `user_subscriptions` already has `updated_at` column but no trigger — add trigger
- Do NOT apply to `wallet_transactions` if it exists (has immutability trigger per RESEARCH.md)
- Check existing v1.6 tables for `updated_at` columns and add triggers where missing

**Naming conventions** (follow existing codebase patterns from `20260212000000_add_whop_subscriptions.sql`):
- Table names: snake_case plural
- Column names: snake_case
- Index names: `idx_{table}_{column}`
- Policy names: Descriptive English sentences in quotes
- All TEXT status/type fields: `CHECK (value IN (...))` pattern (not ENUMs)
  </action>
  <verify>
Run `npx supabase db reset` locally to apply all migrations including the new one. Verify it completes without errors. Then check tables exist:

```bash
npx supabase db reset
# If local Supabase not running, verify SQL syntax:
npx supabase migration list
```

Alternatively, validate SQL syntax by running through `psql` parser or reviewing for syntax errors manually.
  </verify>
  <done>
Migration file exists at `supabase/migrations/20260213000000_content_intelligence.sql` containing: 6 CREATE TABLE statements (scraped_videos, trending_sounds, analysis_results, outcomes, rule_library, usage_tracking), all with RLS enabled, appropriate SELECT/INSERT/UPDATE policies using `(SELECT auth.uid())` pattern, performance indexes on user_id and query columns, partial indexes for soft-deleted rows, `updated_at` triggers on all tables, and soft delete columns (`archived_at` on scraped_videos, `deleted_at` on analysis_results and outcomes).
  </done>
</task>

<task type="auto">
  <name>Task 2: Regenerate TypeScript database types</name>
  <files>src/types/database.types.ts</files>
  <action>
After the migration file is created, regenerate TypeScript types from the schema. Two approaches depending on whether local Supabase is running:

**Approach A (local Supabase running):**
```bash
npx supabase db reset
npx supabase gen types typescript --local > src/types/database.types.ts
```

**Approach B (local Supabase NOT running — manual type definitions):**
If `supabase gen types typescript --local` fails because local Supabase is not running, manually update `src/types/database.types.ts` to add the new table types. Read the existing file to understand the current structure, then add type definitions for all 6 new tables matching the migration schema exactly.

Each table needs:
- `Row` type (all columns as they come from SELECT)
- `Insert` type (columns for INSERT, optional where defaults exist)
- `Update` type (all columns optional for partial updates)

Key type mappings from PostgreSQL to TypeScript:
- `UUID` → `string`
- `TEXT` → `string`
- `TEXT NOT NULL CHECK (... IN (...))` → union type (e.g., `'daily' | 'monthly'`)
- `BIGINT` → `number`
- `INTEGER` → `number`
- `NUMERIC(x,y)` → `number`
- `BOOLEAN` → `boolean`
- `TIMESTAMPTZ` → `string` (ISO 8601)
- `DATE` → `string`
- `JSONB` → `Json` (Supabase's generic Json type)
- `TEXT[]` → `string[]`
- Nullable columns → `type | null`

After adding types, run `npx tsc --noEmit` to verify zero compilation errors.
  </action>
  <verify>
```bash
npx tsc --noEmit
```

Must complete with zero errors. Also verify the new tables appear in the types file:

```bash
grep -c "scraped_videos\|trending_sounds\|analysis_results\|outcomes\|rule_library\|usage_tracking" src/types/database.types.ts
```

Should return a count > 0 for each table name.
  </verify>
  <done>
`src/types/database.types.ts` contains type definitions for all 6 new tables (scraped_videos, trending_sounds, analysis_results, outcomes, rule_library, usage_tracking) with Row, Insert, and Update types. `npx tsc --noEmit` passes with zero errors.
  </done>
</task>

</tasks>

<verification>
1. Migration file exists at `supabase/migrations/20260213000000_content_intelligence.sql`
2. Migration contains 6 CREATE TABLE statements
3. Every table has `ALTER TABLE ... ENABLE ROW LEVEL SECURITY`
4. Every RLS policy on user-scoped tables uses `(SELECT auth.uid())` pattern
5. `scraped_videos` has `archived_at`, `analysis_results` and `outcomes` have `deleted_at`
6. Every table has `created_at` and `updated_at` columns
7. `update_updated_at_column()` trigger function is created and applied to all tables
8. `src/types/database.types.ts` includes types for all 6 new tables
9. `npx tsc --noEmit` passes with zero errors
</verification>

<success_criteria>
- Migration SQL is syntactically valid and can be applied via `supabase db push` or `supabase db reset`
- All 6 tables (scraped_videos, trending_sounds, analysis_results, outcomes, rule_library, usage_tracking) are defined with correct columns, constraints, and defaults
- RLS policies enforce: public read for scraped_videos (non-archived), trending_sounds, and active rules; user-scoped read/write for analysis_results, outcomes, and usage_tracking
- TypeScript types compile with zero errors and include all 6 new tables
</success_criteria>

<output>
After completion, create `.planning/phases/01-database-foundation/01-01-SUMMARY.md`
</output>
