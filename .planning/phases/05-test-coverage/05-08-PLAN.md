---
phase: 05-test-coverage
plan: 08
type: execute
wave: 3
depends_on: ["05-01", "05-02", "05-03", "05-04", "05-05", "05-06", "05-07"]
files_modified:
  - src/lib/engine/__tests__/pipeline.test.ts
autonomous: true

must_haves:
  truths:
    - "Happy path pipeline produces PipelineResult with all stage outputs populated"
    - "DeepSeek failure triggers Gemini fallback — deepseekResult is still populated (fallback shape)"
    - "Gemini failure throws (critical stage — pipeline cannot proceed)"
    - "Pipeline produces warnings array when non-critical stages fail"
    - "`pnpm test:coverage` passes with >80% coverage on all engine modules"
  artifacts:
    - path: "src/lib/engine/__tests__/pipeline.test.ts"
      provides: "Integration tests for full pipeline with 4 scenarios"
      min_lines: 100
  key_links:
    - from: "src/lib/engine/__tests__/pipeline.test.ts"
      to: "src/lib/engine/pipeline.ts"
      via: "import runPredictionPipeline"
      pattern: "import.*runPredictionPipeline.*pipeline"
---

<objective>
Write integration tests for pipeline.ts covering happy path, DeepSeek failure with fallback, Gemini failure, and ML weight redistribution. Then verify >80% coverage across all engine modules.

Purpose: Integration tests verify the wiring between all pipeline stages. Coverage gate ensures no untested code paths.
Output: pipeline.test.ts with 4 scenarios + coverage passing at >80%.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-test-coverage/05-RESEARCH.md
@.planning/phases/05-test-coverage/05-01-SUMMARY.md
@src/lib/engine/pipeline.ts
@src/lib/engine/types.ts
@src/lib/engine/aggregator.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests for pipeline — 4 scenarios</name>
  <files>src/lib/engine/__tests__/pipeline.test.ts</files>
  <action>
Create `src/lib/engine/__tests__/pipeline.test.ts`.

**Mocking strategy for integration tests**: Mock external SDKs but use real internal engine modules. This tests the wiring.

Mocks needed:
- `vi.mock("@/lib/logger", ...)` — createLogger returns stub
- `vi.mock("@sentry/nextjs", ...)` — captureException, addBreadcrumb as vi.fn()
- `vi.mock("@/lib/supabase/service", ...)` — createServiceClient returns a chainable mock that supports from().select().eq().is().not().gte().order().limit() (all return this), with terminal methods (await) returning { data: [], error: null }. Also storage.from().download() for ML weights.
- `vi.mock("@/lib/cache", ...)` — createCache returns { get: vi.fn(() => null), set: vi.fn(), invalidate: vi.fn() }
- `vi.mock("nanoid", ...)` — `nanoid: vi.fn(() => "test-req-id")`
- `vi.mock("openai", ...)` — Mock constructor returns object with `chat.completions.create` as configurable vi.fn()
- `vi.mock("@google/genai", ...)` — Mock GoogleGenAI constructor returns object with `models.generateContent` as configurable vi.fn()
- `vi.mock("node:fs", ...)` — `promises.readFile` returns calibration JSON; `readFileSync` returns training data JSON
- `vi.mock("node:path", ...)` — join and dirname return mock paths

Import `runPredictionPipeline` from `../pipeline` and factories from `./factories`.

Get references to the mock functions so they can be reconfigured per test:
```typescript
import OpenAI from "openai";
import { GoogleGenAI } from "@google/genai";
const mockGeminiGenerate = vi.fn();
const mockDeepSeekCreate = vi.fn();
```

Set up the default mock responses in `beforeEach`:
- Gemini `models.generateContent` returns `{ text: JSON.stringify(makeGeminiAnalysis()), usageMetadata: { promptTokenCount: 500, candidatesTokenCount: 300 } }`
- DeepSeek `chat.completions.create` returns `{ choices: [{ message: { content: JSON.stringify(makeDeepSeekReasoning()) } }], usage: { prompt_tokens: 1000, completion_tokens: 500 } }`
- Supabase rule_library query returns `{ data: [], error: null }` (no rules — simplifies integration test)
- Supabase trending_sounds query returns `{ data: [], error: null }`
- Supabase creator_profiles query returns `{ data: null, error: null }`

Build a standard test input:
```typescript
const input = {
  input_mode: "text" as const,
  content_text: "What if you could double your engagement? #viral #trending",
  content_type: "video" as const,
};
```

**Scenario 1 — Happy path (all stages succeed):**
All mocks configured to succeed. Call `runPredictionPipeline(input)`. Assert:
- result.payload.input_mode === "text"
- result.payload.hashtags includes "#viral"
- result.geminiResult.analysis has 5 factors
- result.deepseekResult is not null
- result.deepseekResult.reasoning has behavioral_predictions
- result.timings is a non-empty array
- result.total_duration_ms > 0
- result.warnings is empty array (no failures)
- result.requestId is a string

**Scenario 2 — DeepSeek failure, Gemini fallback:**
Mock DeepSeek `chat.completions.create` to throw `new Error("503 Service Unavailable")` for all retries (MAX_RETRIES + 1 = 3 times). The Gemini fallback (inside deepseek.ts) uses `GoogleGenAI.models.generateContent` — ensure THAT mock returns valid DeepSeekReasoning JSON.

Call `runPredictionPipeline(input)`. Assert:
- result.deepseekResult is NOT null (Gemini fallback produced it)
- result.deepseekResult.reasoning has all required fields
- result.warnings may contain DeepSeek-related warning OR may be empty (the fallback inside deepseek.ts catches the error before pipeline sees it)

**Scenario 3 — Gemini failure (critical stage):**
Mock Gemini `models.generateContent` to throw `new Error("Gemini API unavailable")`.
Call `runPredictionPipeline(input)`. Assert: the call throws/rejects (Gemini is the critical stage — pipeline cannot proceed).

**Scenario 4 — Non-critical stage failures (rules, trends, creator all fail):**
Configure Supabase mock to throw on from("rule_library"), from("trending_sounds"), from("creator_profiles"). Keep Gemini and DeepSeek working.
Call `runPredictionPipeline(input)`. Assert:
- result is not null (pipeline completes)
- result.warnings.length > 0 (failures logged as warnings)
- result.ruleResult equals default (rule_score: 50, matched_rules: [])
- result.trendEnrichment equals default (trend_score: 0)

Note: The Supabase mock needs to be smart enough to return different results for different table names. Build a from() mock that checks the table name argument:
```typescript
const mockFrom = vi.fn((table: string) => {
  if (table === "rule_library") throw new Error("DB error");
  // ... return chainable for others
});
```
  </action>
  <verify>Run `pnpm test src/lib/engine/__tests__/pipeline.test.ts` — all 4 scenarios pass.</verify>
  <done>Pipeline integration tests cover: happy path, DeepSeek failure with Gemini fallback, Gemini critical failure throws, non-critical stage graceful degradation</done>
</task>

<task type="auto">
  <name>Task 2: Coverage gate — verify >80% and fix gaps</name>
  <files>src/lib/engine/__tests__/pipeline.test.ts</files>
  <action>
Run `pnpm test:coverage` and check the coverage report.

Expected coverage should be >80% for most modules given the unit tests from plans 02-07 and integration tests from this plan.

**If any module is below 80%:**

1. Check the coverage report for uncovered lines/branches.
2. Add targeted tests to the relevant test file to cover the gap. Common gaps:
   - Error handling catch blocks (throw errors in mocks to hit them)
   - Edge case branches (null checks, empty arrays)
   - Default/fallback paths

**Specific modules likely needing attention:**
- `creator.ts` — not directly unit-tested. The pipeline integration test should cover the happy path through fetchCreatorContext. If coverage is low, add a few targeted tests in pipeline.test.ts or create a minimal creator.test.ts.
- `trends.ts` — also tested indirectly through pipeline. May need targeted tests.
- `gemini.ts` — tested indirectly through pipeline. If below 80%, add a gemini.test.ts with mock tests.

**If coverage thresholds pass:** Great, mark task complete.

**To check individual file coverage:** Look at the text output table from `pnpm test:coverage`. Each file in `src/lib/engine/` will show line, branch, function, statement coverage.

The threshold configuration in vitest.config.ts enforces 80% globally. If individual files are below 80% but the aggregate is above, the thresholds still pass. The research config uses global thresholds, not per-file.

**If the global threshold fails:** Identify the lowest-coverage file and add tests for its uncovered paths.

Note: If `pnpm test:coverage` is failing because of test errors (not threshold), fix the test errors first.
  </action>
  <verify>Run `pnpm test:coverage` — exits 0 with all coverage thresholds met (>80% lines, functions, branches, statements for engine modules).</verify>
  <done>`pnpm test:coverage` exits 0. Coverage report shows >80% for all metrics on `src/lib/engine/` files.</done>
</task>

</tasks>

<verification>
1. `pnpm test` exits 0 — all tests pass
2. `pnpm test:coverage` exits 0 — coverage thresholds met
3. Pipeline integration tests cover 4 scenarios: happy path, DeepSeek failure, Gemini failure, non-critical failures
4. Coverage report shows >80% for lines, functions, branches, statements
5. No uncovered critical paths in engine modules
</verification>

<success_criteria>
Pipeline is integration-tested with all failure scenarios. Coverage exceeds 80% across all engine modules. `pnpm test:coverage` passes in CI.
</success_criteria>

<output>
After completion, create `.planning/phases/05-test-coverage/05-08-SUMMARY.md`
</output>
