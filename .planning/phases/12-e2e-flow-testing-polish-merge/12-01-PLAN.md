---
phase: 12-e2e-flow-testing-polish-merge
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark.ts
  - scripts/benchmark-results.json
autonomous: true

must_haves:
  truths:
    - "Benchmark script runs 50 diverse content samples through the full prediction pipeline and records results"
    - "Each sample produces a valid PredictionResult with overall_score, factors, behavioral_predictions, and confidence"
    - "Benchmark output includes score distribution, average latency, cost summary, and confidence breakdown"
    - "v2 results are compared against v1 baseline (score distribution shift, new fields present)"
  artifacts:
    - path: "scripts/benchmark.ts"
      provides: "E2E benchmark script running samples through pipeline + aggregator"
      min_lines: 150
    - path: "scripts/benchmark-results.json"
      provides: "Persisted benchmark output with per-sample results and summary statistics"
  key_links:
    - from: "scripts/benchmark.ts"
      to: "src/lib/engine/pipeline.ts"
      via: "direct import of runPredictionPipeline"
      pattern: "runPredictionPipeline"
    - from: "scripts/benchmark.ts"
      to: "src/lib/engine/aggregator.ts"
      via: "direct import of aggregateScores"
      pattern: "aggregateScores"
---

<objective>
Create an accuracy benchmarking script that runs 50 diverse content samples through the full v2 prediction pipeline and produces a comprehensive results report.

Purpose: Validates that the complete v2 engine (Gemini + DeepSeek + Rules + Trends + Aggregator) produces reasonable, consistent results across varied content types before shipping.
Output: `scripts/benchmark.ts` runnable via `npx tsx scripts/benchmark.ts`, producing `scripts/benchmark-results.json` with per-sample results and aggregate statistics.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/lib/engine/pipeline.ts
@src/lib/engine/aggregator.ts
@src/lib/engine/types.ts
@src/lib/engine/normalize.ts
@scripts/analyze-dataset.ts
@scripts/extract-training-data.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark script with 50 diverse content samples</name>
  <files>scripts/benchmark.ts</files>
  <action>
Create `scripts/benchmark.ts` that:

1. **Setup**: Load .env.local via dotenv (same pattern as extract-training-data.ts and analyze-dataset.ts). Import `runPredictionPipeline` from `../src/lib/engine/pipeline` and `aggregateScores` from `../src/lib/engine/aggregator`. Since these use path aliases (@/), configure tsx with tsconfig-paths or use relative imports.

2. **Sample corpus**: Define 50 hardcoded content samples covering diversity:
   - 15 text-only samples across niches (comedy, dance, cooking, fitness, fashion, beauty, tech, education, storytelling, lifehack, motivation, ASMR, gaming, music, pets)
   - 15 text samples with different content_types (video, image, text, carousel)
   - 10 text samples with hashtags (popular, niche-specific, saturated like #fyp)
   - 5 text samples designed to score high (strong hooks, emotional, trending topics)
   - 5 text samples designed to score low (no hook, generic, wall of text)

   Each sample is an object matching AnalysisInput:
   ```typescript
   {
     input_mode: "text" as const,
     content_text: "...",
     content_type: "video",
     niche: "comedy",  // optional
   }
   ```

   NOTE: Only use input_mode "text" -- do NOT test tiktok_url or video_upload modes (those require external services). The benchmark tests the engine pipeline, not input acquisition.

3. **Execution**: Run each sample through `runPredictionPipeline(sample)` then `aggregateScores(pipelineResult)`. Wrap in try/catch per sample -- failures should be logged but not stop the benchmark. Add a 2s delay between samples to avoid rate-limiting external APIs (Gemini, DeepSeek).

4. **Result collection**: For each sample, record:
   - `sample_index`, `niche`, `content_type`
   - `overall_score`, `confidence`, `confidence_label`
   - `behavioral_score`, `gemini_score`, `rule_score`, `trend_score`
   - `factor_scores`: array of {name, score} from factors
   - `latency_ms`, `cost_cents`
   - `warnings`: array of strings
   - `has_behavioral_predictions`: boolean (behavioral_predictions has non-zero values)
   - `has_factors`: boolean (factors array length > 0)
   - `has_suggestions`: boolean (suggestions array length > 0)
   - `error`: null or error message string

5. **Summary statistics**: After all samples complete, compute:
   - `total_samples`, `successful`, `failed`
   - Score distribution: min, max, mean, median, stddev of overall_score
   - Confidence distribution: count per label (HIGH, MEDIUM, LOW)
   - Average latency_ms, average cost_cents
   - Completeness: % with behavioral_predictions, % with factors, % with suggestions
   - Warning frequency: count of samples with each unique warning type

6. **Output**: Write complete results (per-sample + summary) to `scripts/benchmark-results.json`. Print summary to console with formatted table.

7. **v2 vs v1 comparison notes**: The script should print a text comparison section noting:
   - v2 produces behavioral_predictions (v1 did not)
   - v2 has 5 TikTok-aligned factors vs v1's generic factors
   - v2 has feature_vector backbone (v1 did not)
   - v2 uses dynamic weight selection (v1 used fixed weights)
   - Score range comparison: where most scores cluster

Important implementation details:
- Use `process.exit(0)` at end to avoid hanging on open Supabase connections
- Use the same dotenv + Supabase pattern as existing scripts in the scripts/ directory
- Handle the case where API keys are missing gracefully (error message + skip)
- Print progress as "Processing sample X/50..." to console
  </action>
  <verify>
Run `npx tsx scripts/benchmark.ts` -- it should process all 50 samples (some may fail due to API rate limits, that's OK), produce `scripts/benchmark-results.json`, and print summary to console.
  </verify>
  <done>
Benchmark script runs 50 samples through the full pipeline. Results JSON file exists with per-sample data and summary statistics. Console output shows score distribution and completeness metrics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add benchmark npm script and validate results</name>
  <files>package.json, scripts/benchmark-results.json</files>
  <action>
1. Add npm script to package.json:
   ```json
   "benchmark": "npx tsx scripts/benchmark.ts"
   ```

2. After the benchmark completes (Task 1 generates the results), validate the benchmark-results.json:
   - Verify the JSON is valid and parseable
   - Check that `summary.successful` > 40 (at least 80% success rate)
   - Check that `summary.score_distribution.mean` is between 20 and 80 (sanity range)
   - Check that `summary.completeness.has_factors_pct` is 100% (every successful result has factors)
   - Check that `summary.completeness.has_behavioral_predictions_pct` > 90%
   - Print validation pass/fail for each check

3. If any validation fails, log the issue but do not throw -- this is informational benchmarking, not a test gate. The goal is visibility into engine behavior, not a pass/fail gate.

Note: The benchmark-results.json should be gitignored (add to .gitignore if not already covered by a general pattern) since it contains runtime data that varies per environment.
  </action>
  <verify>
`npm run benchmark` runs the full benchmark. Validation checks print to console. benchmark-results.json is generated with valid structure.
  </verify>
  <done>
npm run benchmark works end-to-end. Results are validated with sanity checks. Script entry added to package.json.
  </done>
</task>

</tasks>

<verification>
1. `npx tsx scripts/benchmark.ts` completes without crashing
2. `scripts/benchmark-results.json` exists and is valid JSON
3. Summary shows 50 samples attempted with majority successful
4. Each successful sample has: overall_score (0-100), factors (5 items), behavioral_predictions, confidence
5. `npm run benchmark` script works from package.json
</verification>

<success_criteria>
- 50 content samples benchmarked through the full v2 pipeline
- Per-sample results and aggregate statistics persisted to JSON
- Score distribution, confidence breakdown, and completeness metrics visible in console output
- v2 vs v1 comparison notes printed
</success_criteria>

<output>
After completion, create `.planning/phases/12-e2e-flow-testing-polish-merge/12-01-SUMMARY.md`
</output>
