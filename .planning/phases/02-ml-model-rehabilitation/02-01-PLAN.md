---
phase: 02-ml-model-rehabilitation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/engine/ml.ts
autonomous: true

must_haves:
  truths:
    - "trainModel() applies inverse-frequency class weighting so tiers 4 and 5 get proportionally higher gradient signal"
    - "trainModel() logs per-class precision/recall after training on both train and test sets"
    - "featureVectorToMLInput() maps shareability, commentProvocation, emotionalCharge, and saveWorthiness from FeatureVector instead of hardcoded 0.5"
    - "trainModel() accepts an optional structured data parameter (features + labels) instead of only reading from filesystem"
    - "A stratifiedSplit() utility function correctly partitions samples by label with proportional representation"
    - "Class weights are capped at 3x the minimum weight to prevent overfitting to rare classes"
  artifacts:
    - path: "src/lib/engine/ml.ts"
      provides: "Class-weighted training, stratified split, real feature bridge, data-param trainModel"
      contains: "computeClassWeights"
  key_links:
    - from: "src/lib/engine/ml.ts:trainModel"
      to: "src/lib/engine/ml.ts:computeClassWeights"
      via: "weights applied in gradient loop"
      pattern: "classWeight.*error"
    - from: "src/lib/engine/ml.ts:featureVectorToMLInput"
      to: "FeatureVector.shareability"
      via: "real DeepSeek component scores"
      pattern: "fv\\.shareability"
---

<objective>
Add inverse-frequency class weighting, stratified splitting, per-class metrics logging, real feature bridge signals, and a data-parameter overload to trainModel() -- all within ml.ts.

Purpose: Fix the ML model's inability to predict tiers 4 and 5 (caused by class imbalance and 0.5 placeholder features) and prepare trainModel() for dynamic data ingestion from the retrain cron.

Output: A rehabilitated ml.ts where training uses class weights, evaluation logs per-class precision/recall, the feature bridge maps 4 real DeepSeek signals instead of hardcoded 0.5, and trainModel can accept pre-computed training data.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-ml-model-rehabilitation/02-RESEARCH.md
@src/lib/engine/ml.ts
@src/lib/engine/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add class weighting, stratified split, and per-class metrics to ml.ts</name>
  <files>src/lib/engine/ml.ts</files>
  <action>
Add three new functions to ml.ts (place them in a new section above the Training section):

1. `computeClassWeights(labels: number[], numClasses: number): number[]`
   - Formula: `weight_c = total_samples / (num_classes * count_c)`
   - Labels are 1-indexed (1-5). Map `label - 1` to index into counts array.
   - Cap each weight at `3 * min_weight` to prevent overfitting per Pitfall 1 in research.
   - Return array of length numClasses.

2. `stratifiedSplit(features: number[][], labels: number[], testRatio: number, rng: () => number): { train: { features: number[][]; labels: number[] }; test: { features: number[][]; labels: number[] } }`
   - Group indices by label value.
   - Shuffle each group with Fisher-Yates using provided rng.
   - Take `floor(group.length * (1 - testRatio))` for train, rest for test.
   - Return { train, test } with features and labels arrays.

3. `logPerClassMetrics(confusionMatrix: number[][], setName: string): void`
   - For each class 0 to NUM_CLASSES-1:
     - truePositive = confusionMatrix[c][c]
     - precision = TP / column_sum, recall = TP / row_sum
     - Log: `[ml] ${setName} Tier ${c+1}: precision=${(precision*100).toFixed(1)}% recall=${(recall*100).toFixed(1)}% (${rowSum} samples)`

Then modify `trainModel()`:

4. Change signature to accept optional pre-computed data:
   ```typescript
   export async function trainModel(
     input?: string | { trainSet: { features: number[][]; labels: number[] }; testSet: { features: number[][]; labels: number[] }; featureNames: string[] }
   ): Promise<TrainingResult>
   ```
   - If `input` is a string, treat as file path (current behavior).
   - If `input` is an object, use it directly as training data.
   - If `input` is undefined, use TRAINING_DATA_PATH (current default).

5. After loading data, call `computeClassWeights(trainLabels, NUM_CLASSES)` and log the weights:
   ```
   [ml] Class weights: Tier 1=0.808 Tier 2=0.795 ... (capped at 3x min)
   ```

6. In the gradient descent inner loop (line ~217), multiply the error by the class weight for the sample's true class:
   ```typescript
   const classWeight = classWeights[target] ?? 1;
   // Then multiply error by classWeight in both gradB and gradW updates
   ```
   The weighting goes on the ERROR term, not the gradient directly. The existing code computes `error = probs[c] - (c === target ? 1 : 0)`. Multiply this by `classWeight` before accumulating into gradB and gradW.

7. After training completes, call `logPerClassMetrics(trainEval.confusionMatrix, "Train")` and `logPerClassMetrics(testEval.confusionMatrix, "Test")`.

8. Also add a `stratifiedSplit` export (it will be used by the retrain cron in Plan 02-03).
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors in ml.ts.
    Grep for `computeClassWeights` and `stratifiedSplit` and `logPerClassMetrics` in ml.ts to confirm all three functions exist.
    Grep for `classWeight` in the gradient descent loop to confirm weighting is applied.
  </verify>
  <done>
    trainModel() computes and applies inverse-frequency class weights (capped at 3x min) during gradient descent, logs per-class precision/recall for both train and test sets, accepts either a file path or structured data object, and exports a stratifiedSplit utility.
  </done>
</task>

<task type="auto">
  <name>Task 2: Replace hardcoded 0.5 features in featureVectorToMLInput with real signals</name>
  <files>src/lib/engine/ml.ts</files>
  <action>
In `featureVectorToMLInput()`, replace the 4 hardcoded 0.5 engagement proxies with real FeatureVector signals:

Current (lines 393-396):
```typescript
const shareRate = 0.5;
const commentRate = 0.5;
const likeRate = 0.5;
const saveRate = 0.5;
```

Replace with:
```typescript
// Map DeepSeek component scores (0-10) to 0-1 range as engagement proxies
// shareability -> shareRate, commentProvocation -> commentRate,
// emotionalCharge -> likeRate (emotional engagement proxy),
// saveWorthiness -> saveRate
const shareRate = clamp01((fv.shareability ?? 5) / 10);
const commentRate = clamp01((fv.commentProvocation ?? 5) / 10);
const likeRate = clamp01((fv.emotionalCharge ?? 5) / 10);
const saveRate = clamp01((fv.saveWorthiness ?? 5) / 10);
```

Also update the comment block above these lines to explain the mapping rationale:
```
// Map DeepSeek component scores to engagement rate proxies.
// The model was trained on scraped engagement rates (shares/views, etc.).
// At inference time, we use AI-assessed quality scores as semantic proxies.
// shareability -> shareRate, commentProvocation -> commentRate,
// emotionalCharge -> likeRate, saveWorthiness -> saveRate.
// Default to 5 (mid-range) when DeepSeek result is unavailable.
```

Keep the following defaults as-is (they genuinely have no pipeline equivalent):
- `hasFollowerData = 0`
- `followerTier = 0.5`
- `viewsPerFollower = 0`
- `weekdayPosted = 0.5`
- `hourPosted = 0.5`
  </action>
  <verify>
    Grep ml.ts for `= 0.5` — should find only followerTier, weekdayPosted, hourPosted (3 occurrences). The 4 engagement features should NOT have `= 0.5`.
    Run `npx tsc --noEmit` — no type errors.
  </verify>
  <done>
    featureVectorToMLInput() uses fv.shareability, fv.commentProvocation, fv.emotionalCharge, and fv.saveWorthiness instead of hardcoded 0.5. Only 3 genuinely unavailable features retain defaults.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `computeClassWeights`, `stratifiedSplit`, `logPerClassMetrics` all exist as functions in ml.ts
3. trainModel() signature accepts `string | object | undefined` parameter
4. Gradient loop multiplies error by classWeight
5. featureVectorToMLInput() references fv.shareability, fv.commentProvocation, fv.emotionalCharge, fv.saveWorthiness
6. Only 3 features (followerTier, weekdayPosted, hourPosted) still use 0.5 defaults
</verification>

<success_criteria>
- All new functions type-check and are exported where needed (stratifiedSplit)
- trainModel() logs class weights and per-class precision/recall
- Feature bridge maps 4 real DeepSeek signals — no hardcoded 0.5 for engagement proxies
- trainModel() can accept pre-computed data for the retrain cron (Plan 02-03)
</success_criteria>

<output>
After completion, create `.planning/phases/02-ml-model-rehabilitation/02-01-SUMMARY.md`
</output>
