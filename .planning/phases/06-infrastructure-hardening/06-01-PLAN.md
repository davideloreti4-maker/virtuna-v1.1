---
phase: 06-infrastructure-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/cache.ts
  - src/app/api/analyze/route.ts
  - src/lib/engine/pipeline.ts
  - src/lib/engine/deepseek.ts
autonomous: true

must_haves:
  truths:
    - "Free tier users get 429 after 5 analyses per day"
    - "Starter tier users get 429 after 50 analyses per day"
    - "Pro tier users never get rate limited"
    - "Rule library DB queries are cached for 1 hour"
    - "Trending sounds DB queries are cached for 5 minutes"
    - "Scraped videos DB queries are cached for 15 minutes"
    - "If Gemini or trends fail, pipeline completes with warnings and reduced confidence"
    - "DeepSeek circuit breaker retries with 1s, 3s, 9s exponential backoff"
    - "Circuit breaker enters half-open state to probe recovery"
    - "TikTok URLs are validated against tiktok.com domain pattern"
    - "Content text rejects inputs over 10,000 characters"
  artifacts:
    - path: "src/lib/cache.ts"
      provides: "Generic in-memory TTL cache"
      exports: ["createCache", "CacheEntry"]
    - path: "src/app/api/analyze/route.ts"
      provides: "Rate limiting enforcement before pipeline execution"
      contains: "usage_tracking"
    - path: "src/lib/engine/pipeline.ts"
      provides: "Partial failure mode with warnings"
      contains: "allSettled"
    - path: "src/lib/engine/deepseek.ts"
      provides: "Exponential backoff circuit breaker with half-open state"
      contains: "halfOpen"
  key_links:
    - from: "src/app/api/analyze/route.ts"
      to: "src/lib/cache.ts"
      via: "import createCache"
      pattern: "import.*createCache.*cache"
    - from: "src/lib/engine/pipeline.ts"
      to: "src/lib/cache.ts"
      via: "cached rule/trend/video queries"
      pattern: "import.*cache"
    - from: "src/app/api/analyze/route.ts"
      to: "user_subscriptions table"
      via: "tier lookup for rate limiting"
      pattern: "user_subscriptions"
---

<objective>
Harden the prediction engine for production: add rate limiting by subscription tier, in-memory caching for frequently-queried DB tables, partial failure recovery in the pipeline, and an improved circuit breaker with exponential backoff.

Purpose: Prevent abuse, reduce Supabase query load, improve resilience when external APIs (Gemini/DeepSeek) have transient failures, and validate inputs before expensive pipeline execution.

Output: 4 files modified/created delivering INFRA-01 through INFRA-04.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/lib/engine/pipeline.ts
@src/lib/engine/deepseek.ts
@src/lib/engine/rules.ts
@src/lib/engine/trends.ts
@src/app/api/analyze/route.ts
@src/lib/whop/config.ts
@src/lib/whop/subscription.ts
@src/lib/engine/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TTL cache module + wire cached DB queries into rules and trends</name>
  <files>
    src/lib/cache.ts
    src/lib/engine/rules.ts
    src/lib/engine/trends.ts
  </files>
  <action>
Create `src/lib/cache.ts` — a generic in-memory TTL cache utility:

```typescript
interface CacheEntry<T> {
  data: T;
  expiresAt: number;
}

function createCache<T>(ttlMs: number) {
  const store = new Map<string, CacheEntry<T>>();

  return {
    get(key: string): T | null {
      const entry = store.get(key);
      if (!entry) return null;
      if (Date.now() > entry.expiresAt) {
        store.delete(key);
        return null;
      }
      return entry.data;
    },
    set(key: string, data: T): void {
      store.set(key, { data, expiresAt: Date.now() + ttlMs });
    },
    invalidate(key: string): void {
      store.delete(key);
    },
    clear(): void {
      store.clear();
    },
    get size(): number {
      return store.size;
    },
  };
}
```

Export `createCache` and `CacheEntry` type. Keep it simple — no LRU eviction (dataset sizes are small: ~50 trending sounds, ~200 scraped videos, ~30 rules).

**Wire into `src/lib/engine/rules.ts`** (INFRA-02):
- Import `createCache` from `@/lib/cache`
- Create module-level cache: `const rulesCache = createCache<Rule[]>(60 * 60 * 1000)` (1 hour TTL)
- In `loadActiveRules()`, check cache first using key `rules:${platform ?? "all"}`. If hit, return cached. If miss, query DB, cache result, return.
- Do NOT change `scoreContentAgainstRules` — scoring always runs fresh with cached rules.

**Wire into `src/lib/engine/trends.ts`** (INFRA-02):
- Import `createCache` from `@/lib/cache`
- Create two module-level caches:
  - `const soundsCache = createCache<TrendingSound[]>(5 * 60 * 1000)` (5 min TTL)
  - `const videosCache = createCache<ScrapedVideo[]>(15 * 60 * 1000)` (15 min TTL)
- Define minimal types for cached data: `interface TrendingSound { sound_name: string; velocity_score: number; trend_phase: string | null }` and `interface ScrapedVideo { hashtags: string[]; views: number }`.
- In `enrichWithTrends()`, check `soundsCache.get("trending_sounds")` before the Supabase query. On miss, query and cache.
- Similarly, check `videosCache.get("recent_videos")` before the scraped_videos query. On miss, query and cache.
- The rest of the enrichment logic stays the same — it just operates on cached or fresh data.
  </action>
  <verify>
- `pnpm tsc --noEmit` passes (no type errors)
- `src/lib/cache.ts` exports `createCache`
- `rules.ts` imports from `@/lib/cache` and uses `rulesCache.get/set`
- `trends.ts` imports from `@/lib/cache` and uses `soundsCache.get/set` + `videosCache.get/set`
  </verify>
  <done>
- TTL cache module exists at `src/lib/cache.ts` with get/set/invalidate/clear API
- Rules cached for 1 hour — second call within TTL returns cached data without DB query
- Trending sounds cached for 5 minutes
- Scraped videos cached for 15 minutes
  </done>
</task>

<task type="auto">
  <name>Task 2: Rate limiting + input validation in API route</name>
  <files>
    src/app/api/analyze/route.ts
  </files>
  <action>
Add rate limiting (INFRA-01) and enhanced input validation (INFRA-04) to the analyze API route.

**Rate limiting logic** — add BEFORE the SSE stream setup, after user authentication:

1. Define tier limits as a const object:
```typescript
const DAILY_LIMITS: Record<string, number> = {
  free: 5,
  starter: 50,
  pro: Infinity,  // unlimited
};
```

2. Query the user's subscription tier:
```typescript
const { data: subscription } = await supabase
  .from("user_subscriptions")
  .select("virtuna_tier")
  .eq("user_id", user.id)
  .single();
const tier = (subscription?.virtuna_tier as string) || "free";
```

3. Query today's usage count:
```typescript
const today = new Date().toISOString().split("T")[0]!;
const service = createServiceClient();
const { data: usage } = await service
  .from("usage_tracking")
  .select("analysis_count")
  .eq("user_id", user.id)
  .eq("period_start", today)
  .eq("period_type", "daily")
  .single();
const currentCount = usage?.analysis_count ?? 0;
```

4. Check against limit:
```typescript
const limit = DAILY_LIMITS[tier] ?? DAILY_LIMITS.free;
if (currentCount >= limit) {
  return Response.json(
    { error: "Daily analysis limit reached", limit, tier, reset: "midnight UTC" },
    { status: 429 }
  );
}
```

Move the `createServiceClient()` call BEFORE the stream (it's used for rate limit check AND inside the stream). Pass `service` into the stream closure.

**Input validation** (INFRA-04) — add additional validation after `request.json()` but before `AnalysisInputSchema.parse()`:

1. Content text length check (already in Zod `.max(10000)`, but add early rejection before parse for better error message):
```typescript
if (body.content_text && typeof body.content_text === 'string' && body.content_text.length > 10000) {
  return Response.json(
    { error: "Content text exceeds maximum length of 10,000 characters" },
    { status: 400 }
  );
}
```

2. TikTok URL format validation:
```typescript
if (body.input_mode === 'tiktok_url' && body.tiktok_url) {
  const tiktokPattern = /^https?:\/\/(www\.|vm\.)?tiktok\.com\//;
  if (!tiktokPattern.test(body.tiktok_url)) {
    return Response.json(
      { error: "Invalid TikTok URL. Must be a tiktok.com link." },
      { status: 400 }
    );
  }
}
```

3. Video URL format check:
```typescript
if (body.input_mode === 'video_upload' && body.video_storage_path) {
  if (typeof body.video_storage_path !== 'string' || body.video_storage_path.length === 0) {
    return Response.json(
      { error: "Invalid video storage path" },
      { status: 400 }
    );
  }
}
```

**Important:** Keep the existing `usage_tracking` upsert AFTER the pipeline completes (it increments the count AFTER a successful analysis, not before). The rate limit check reads the current count; the upsert at the end increments it.
  </action>
  <verify>
- `pnpm tsc --noEmit` passes
- Rate limit returns 429 with `{ error, limit, tier, reset }` body
- TikTok URL validation rejects non-tiktok.com URLs
- Content length validated before Zod parse
- `createServiceClient()` moved before stream, used in both rate limit check and stream
  </verify>
  <done>
- Free tier users are blocked after 5 analyses per day with 429 status
- Starter tier users are blocked after 50 analyses per day with 429 status
- Pro tier users are never rate limited
- Non-TikTok URLs rejected with 400 for tiktok_url input mode
- Content over 10K chars rejected with 400
- Empty/invalid video storage paths rejected with 400
  </done>
</task>

<task type="auto">
  <name>Task 3: Exponential backoff circuit breaker + partial pipeline failure recovery</name>
  <files>
    src/lib/engine/deepseek.ts
    src/lib/engine/pipeline.ts
  </files>
  <action>
**Circuit breaker rewrite in `src/lib/engine/deepseek.ts`** (INFRA-03):

Replace the current flat-cooldown circuit breaker with exponential backoff + half-open state:

1. Replace circuit breaker state variables:
```typescript
// Circuit breaker state (INFRA-03: exponential backoff with half-open)
interface CircuitBreakerState {
  status: 'closed' | 'open' | 'half-open';
  consecutiveFailures: number;
  nextRetryAt: number; // timestamp when half-open probe is allowed
  backoffIndex: number; // 0, 1, 2 -> maps to 1s, 3s, 9s
}

const BACKOFF_SCHEDULE_MS = [1_000, 3_000, 9_000]; // 1s, 3s, 9s exponential
const FAILURE_THRESHOLD = 3;

let breaker: CircuitBreakerState = {
  status: 'closed',
  consecutiveFailures: 0,
  nextRetryAt: 0,
  backoffIndex: 0,
};
```

2. Replace `isCircuitOpen()`:
```typescript
function isCircuitOpen(): boolean {
  if (breaker.status === 'closed') return false;
  if (breaker.status === 'open') {
    if (Date.now() >= breaker.nextRetryAt) {
      // Transition to half-open: allow ONE probe request
      breaker.status = 'half-open';
      return false;
    }
    return true;
  }
  // half-open: allow the probe through
  return false;
}
```

3. Replace `recordFailure()`:
```typescript
function recordFailure(): void {
  breaker.consecutiveFailures++;
  if (breaker.status === 'half-open' || breaker.consecutiveFailures >= FAILURE_THRESHOLD) {
    // Open the circuit with exponential backoff
    const backoffMs = BACKOFF_SCHEDULE_MS[breaker.backoffIndex] ?? BACKOFF_SCHEDULE_MS[BACKOFF_SCHEDULE_MS.length - 1]!;
    breaker.status = 'open';
    breaker.nextRetryAt = Date.now() + backoffMs;
    breaker.backoffIndex = Math.min(breaker.backoffIndex + 1, BACKOFF_SCHEDULE_MS.length - 1);
    console.warn(
      `[DeepSeek] Circuit breaker OPEN. Next retry in ${backoffMs}ms (backoff level ${breaker.backoffIndex})`
    );
  }
}
```

4. Replace `recordSuccess()`:
```typescript
function recordSuccess(): void {
  // Full reset on success — circuit closes, backoff resets
  breaker = {
    status: 'closed',
    consecutiveFailures: 0,
    nextRetryAt: 0,
    backoffIndex: 0,
  };
}
```

5. Remove the old module-level variables: `consecutiveFailures`, `circuitOpenUntil`, `COOLDOWN_MS`.

6. Update the export: still export `isCircuitOpen` for external checks, keep `DEEPSEEK_MODEL`.

**Partial pipeline failure in `src/lib/engine/pipeline.ts`** (Success Criteria 3):

The current pipeline uses strict `Promise.all` with try/catch re-throws in every stage. Change to allow partial failure for non-critical stages (Gemini failure is still fatal since behavioral needs it; DeepSeek is already handled by circuit breaker returning null).

Modify Wave 1 to use `Promise.allSettled` for non-critical stages:

1. Keep Gemini as critical (throw on failure — the pipeline can't produce meaningful results without Gemini's 5 factors).
2. Keep DeepSeek as critical (throw on failure per the existing strict mode, but the circuit breaker already handles degradation by returning null).
3. Make **Creator Context**, **Rule Scoring**, and **Trend Enrichment** non-critical — they add signal but aren't essential.

Implementation approach for Wave 1:
```typescript
// Stage 3: Gemini (CRITICAL — still uses try/throw)
const geminiResult = await timed("gemini_analysis", timings, async () => {
  try {
    return await analyzeWithGemini(validated);
  } catch (error) {
    throw new Error(`Analysis failed: Gemini content analysis — ${error instanceof Error ? error.message : String(error)}`);
  }
});

// Stages 4-6: Non-critical (use try/catch with fallback + warning)
const warnings: string[] = [];

const audioResult = await timed("audio_analysis", timings, async () => null);

let creatorContext: CreatorContext = { /* default empty context */ };
try {
  creatorContext = await timed("creator_context", timings, () =>
    fetchCreatorContext(supabase, payload.creator_handle, payload.niche)
  );
} catch (error) {
  warnings.push(`Creator context unavailable: ${error instanceof Error ? error.message : String(error)}`);
  timings.push({ stage: "creator_context", duration_ms: 0 });
}

let ruleResult: RuleScoreResult = { rule_score: 50, matched_rules: [] };
try {
  ruleResult = await timed("rule_scoring", timings, async () => {
    const rules = await loadActiveRules(supabase, payload.content_type);
    return scoreContentAgainstRules(payload.content_text, rules);
  });
} catch (error) {
  warnings.push(`Rule scoring unavailable: ${error instanceof Error ? error.message : String(error)}`);
  timings.push({ stage: "rule_scoring", duration_ms: 0 });
}
```

Same approach for Wave 2 — keep DeepSeek critical, make trend enrichment non-critical:
```typescript
let trendEnrichment: TrendEnrichment = { trend_score: 0, matched_trends: [], trend_context: "Trend data unavailable." };
try {
  trendEnrichment = await timed("trend_enrichment", timings, () =>
    enrichWithTrends(supabase, validated)
  );
} catch (error) {
  warnings.push(`Trend enrichment unavailable: ${error instanceof Error ? error.message : String(error)}`);
  timings.push({ stage: "trend_enrichment", duration_ms: 0 });
}
```

**IMPORTANT architecture change:** Run Wave 1 non-critical stages in parallel with Gemini (still parallel, just don't fail the whole wave):
- Run Gemini via its own `timed()` call
- Run Creator + Rules + Audio as `Promise.allSettled` with individual try/catch
- This preserves parallel execution while allowing individual non-critical stages to fail gracefully

Add `warnings` to the `PipelineResult` interface:
```typescript
export interface PipelineResult {
  // ... existing fields
  warnings: string[]; // Pipeline-level warnings from partial failures
}
```

Update `aggregateScores` in the aggregator to merge pipeline warnings with DeepSeek warnings. But wait — aggregator.ts is NOT in files_modified for this plan. Instead, pass warnings through PipelineResult and let the aggregator naturally pick them up via `pipelineResult.warnings` in a future phase, OR handle it in the API route by merging `pipelineResult.warnings` into the final result after `aggregateScores()`. Do the API route approach to avoid modifying aggregator.ts:

In `route.ts`, after `const result = aggregateScores(pipelineResult)`:
```typescript
// Merge pipeline warnings into result
if (pipelineResult.warnings.length > 0) {
  result.warnings = [...pipelineResult.warnings, ...result.warnings];
}
```

Wait — `PredictionResult` is returned by `aggregateScores` and its `warnings` is readonly-ish. Since `aggregateScores` returns a plain object, we can mutate it. But cleaner: add pipeline warnings in the API route before `send("complete", ...)`. Actually, let's just spread them. In the route:
```typescript
const result = aggregateScores(pipelineResult);
// Prepend pipeline warnings (partial failures) before DeepSeek warnings
const finalResult = {
  ...result,
  warnings: [...pipelineResult.warnings, ...result.warnings],
};
send("complete", finalResult);
```

Update the DB insert to use `finalResult` instead of `result`.
  </action>
  <verify>
- `pnpm tsc --noEmit` passes
- In deepseek.ts: `breaker` object has `status`, `consecutiveFailures`, `nextRetryAt`, `backoffIndex` fields
- In deepseek.ts: `BACKOFF_SCHEDULE_MS` is `[1000, 3000, 9000]`
- In deepseek.ts: `isCircuitOpen()` transitions from `open` to `half-open` when `nextRetryAt` is passed
- In deepseek.ts: `recordSuccess()` fully resets breaker to closed state with `backoffIndex: 0`
- In pipeline.ts: `PipelineResult` has `warnings: string[]` field
- In pipeline.ts: Creator context, rule scoring, and trend enrichment have try/catch with fallback values
- In pipeline.ts: Gemini and DeepSeek still throw on failure (critical stages)
- In route.ts: Pipeline warnings are merged into final result before SSE send
  </verify>
  <done>
- Circuit breaker uses 1s/3s/9s exponential backoff schedule
- Circuit breaker has 3 states: closed (normal), open (blocking), half-open (probe)
- Successful probe in half-open resets circuit to closed with backoffIndex=0
- Failed probe in half-open re-opens circuit with next backoff level
- Pipeline completes even when creator context, rule scoring, or trend enrichment fails
- Partial failures produce specific warning strings in the result
- Gemini and DeepSeek failures still halt the pipeline (critical stages)
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Type check:** `pnpm tsc --noEmit` passes with zero errors
2. **Cache module:** `src/lib/cache.ts` exists, exports `createCache`, used in rules.ts and trends.ts
3. **Rate limiting:** API route queries `user_subscriptions` for tier, checks `usage_tracking` count against limit, returns 429 when exceeded
4. **Input validation:** TikTok URL format validated, content length capped at 10K, video storage path validated
5. **Circuit breaker:** DeepSeek uses exponential backoff (1s, 3s, 9s) with closed/open/half-open states
6. **Partial failure:** Pipeline completes with warnings when non-critical stages fail (creator context, rules, trends)
7. **No regressions:** All existing pipeline behavior preserved for happy path
</verification>

<success_criteria>
- Free: 5/day, Starter: 50/day, Pro: unlimited — enforced with 429 responses
- Rules DB queries cached 1hr, trending_sounds 5min, scraped_videos 15min
- DeepSeek circuit breaker: 1s->3s->9s backoff with half-open probe
- Pipeline completes with warnings when non-critical stages fail
- TikTok URLs validated against tiktok.com domain pattern
- Content text rejects >10K characters with 400
- `pnpm tsc --noEmit` passes
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-hardening/06-01-SUMMARY.md`
</output>
