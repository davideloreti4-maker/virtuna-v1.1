---
phase: 02-competitor-management
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/cron-auth.ts
  - src/app/api/cron/refresh-competitors/route.ts
  - vercel.json
autonomous: true

must_haves:
  truths:
    - "Vercel cron route triggers daily batch re-scraping of all tracked competitors"
    - "Each competitor gets a fresh profile snapshot written to competitor_snapshots"
    - "Scraping failures for individual handles do not block the batch -- failed handles are logged and skipped"
    - "Cron endpoint is protected by CRON_SECRET Bearer token authentication"
  artifacts:
    - path: "src/lib/cron-auth.ts"
      provides: "Cron endpoint authentication utility"
      exports: ["verifyCronAuth"]
    - path: "src/app/api/cron/refresh-competitors/route.ts"
      provides: "GET route handler for daily competitor re-scraping"
      exports: ["GET", "maxDuration"]
    - path: "vercel.json"
      provides: "Cron schedule configuration"
      contains: "refresh-competitors"
  key_links:
    - from: "src/app/api/cron/refresh-competitors/route.ts"
      to: "src/lib/cron-auth.ts"
      via: "verifyCronAuth() as first check"
      pattern: "verifyCronAuth"
    - from: "src/app/api/cron/refresh-competitors/route.ts"
      to: "src/lib/supabase/service.ts"
      via: "createServiceClient() for all DB operations (bypasses RLS)"
      pattern: "createServiceClient"
    - from: "src/app/api/cron/refresh-competitors/route.ts"
      to: "src/lib/scraping/index.ts"
      via: "createScrapingProvider().scrapeProfile() for re-scraping"
      pattern: "createScrapingProvider.*scrapeProfile"
    - from: "vercel.json"
      to: "src/app/api/cron/refresh-competitors/route.ts"
      via: "crons[].path configuration"
      pattern: "/api/cron/refresh-competitors"
---

<objective>
Create the cron infrastructure for daily batch re-scraping of all tracked competitors. Includes a cron auth utility, the cron route handler with per-handle error isolation, and vercel.json cron configuration.

Purpose: Keeps competitor data fresh by collecting daily follower/engagement snapshots, enabling the time-series growth charts in Phase 4.
Output: Protected cron endpoint that scrapes all unique competitor handles, updates profiles, creates daily snapshots, and handles failures gracefully.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation/01-02-SUMMARY.md

# Phase 1 source files (exact APIs to consume)
@src/lib/supabase/service.ts
@src/lib/scraping/types.ts
@src/lib/scraping/index.ts
@supabase/migrations/20260216100000_competitor_tables.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cron auth utility and batch re-scraping route</name>
  <files>
    src/lib/cron-auth.ts
    src/app/api/cron/refresh-competitors/route.ts
  </files>
  <action>
**File 1: `src/lib/cron-auth.ts`**

Create a shared cron authentication utility matching the backend-foundation pattern.

```typescript
import { NextResponse } from "next/server";

export function verifyCronAuth(request: Request): NextResponse | null {
  const authHeader = request.headers.get("authorization");
  const expected = `Bearer ${process.env.CRON_SECRET}`;

  if (!authHeader || authHeader !== expected) {
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  }

  return null; // Auth passed
}
```

Returns `null` when auth passes, a 401 `NextResponse` when it fails. Callers check `if (authError) return authError;`.

**File 2: `src/app/api/cron/refresh-competitors/route.ts`**

Create the batch re-scraping cron route.

**Imports:**
- `NextResponse` from `next/server`
- `revalidatePath` from `next/cache`
- `verifyCronAuth` from `@/lib/cron-auth`
- `createServiceClient` from `@/lib/supabase/service`
- `createScrapingProvider` from `@/lib/scraping`

**Export `maxDuration = 60`** (safe default for Vercel Pro; add comment that 300 is available on Pro with confirmation).

**Export `async function GET(request: Request)`:**

Flow:
1. **Auth check:** `const authError = verifyCronAuth(request); if (authError) return authError;`
2. **Init clients:** `const supabase = createServiceClient()` and `const scraper = createScrapingProvider()`.
3. **Fetch all profiles:** Query `competitor_profiles` for `id, tiktok_handle` (no filter on scrape_status -- re-scrape everything). Service client bypasses RLS.
4. **Early return if empty:** If no profiles, return `NextResponse.json({ refreshed: 0, failed: 0, total: 0 })`.
5. **Iterate with per-handle error isolation:** Loop over profiles. For each profile, wrap in try/catch:

   **Try block:**
   a. `const profileData = await scraper.scrapeProfile(profile.tiktok_handle)`
   b. Update `competitor_profiles` with service client: `.update({ display_name: profileData.displayName, bio: profileData.bio, avatar_url: profileData.avatarUrl, verified: profileData.verified, follower_count: profileData.followerCount, following_count: profileData.followingCount, heart_count: profileData.heartCount, video_count: profileData.videoCount, last_scraped_at: new Date().toISOString(), scrape_status: "success" }).eq("id", profile.id)`.
   c. Upsert into `competitor_snapshots`: `.upsert({ competitor_id: profile.id, follower_count: profileData.followerCount, following_count: profileData.followingCount, heart_count: profileData.heartCount, video_count: profileData.videoCount, snapshot_date: new Date().toISOString().split("T")[0] }, { onConflict: "competitor_id,snapshot_date" })`.
   d. Increment `refreshed` counter.

   **Catch block:**
   a. `console.error(\`[refresh-competitors] Failed for ${profile.tiktok_handle}:\`, error)`
   b. Update profile's `scrape_status` to `"failed"`: `.update({ scrape_status: "failed" }).eq("id", profile.id)`.
   c. Increment `failed` counter.
   d. **Continue to next profile -- do NOT break or rethrow.** This is explicitly required by Success Criteria #4.

6. **Revalidate:** `revalidatePath("/competitors")` after all processing.
7. **Return summary:** `NextResponse.json({ refreshed, failed, total: profiles.length })`.

**Critical details:**
- Use service client for ALL operations (profiles, snapshots). The cron route has no user session.
- Each handle's scrape-and-update is independently try/caught. One failure must not block the batch.
- Snapshot upsert uses `onConflict: "competitor_id,snapshot_date"` to deduplicate (one snapshot per profile per day).
- Do NOT scrape videos in the cron route (daily video refresh is expensive and not required by Phase 2 success criteria -- video scraping happens in the add flow and can be added to cron in Phase 7).
  </action>
  <verify>
Run `npx tsc --noEmit` and confirm no TypeScript errors in `src/lib/cron-auth.ts` and `src/app/api/cron/refresh-competitors/route.ts`. Verify `verifyCronAuth` is exported from cron-auth. Verify `GET` and `maxDuration` are exported from the route.
  </verify>
  <done>
`verifyCronAuth` utility exists at `src/lib/cron-auth.ts`. Cron route exists at `src/app/api/cron/refresh-competitors/route.ts` with per-handle error isolation, profile update + snapshot upsert per handle, and CRON_SECRET Bearer auth protection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create vercel.json with daily cron schedule</name>
  <files>vercel.json</files>
  <action>
Create `vercel.json` at project root with the cron schedule:

```json
{
  "crons": [
    {
      "path": "/api/cron/refresh-competitors",
      "schedule": "0 6 * * *"
    }
  ]
}
```

Schedule `0 6 * * *` = daily at 6:00 AM UTC. This works on both Vercel Hobby and Pro plans (daily frequency).

**Do NOT add any other fields** to vercel.json (no build, framework, or routing config -- Next.js auto-detection handles those).
  </action>
  <verify>
Verify `vercel.json` exists at project root, is valid JSON, and contains the cron entry for `/api/cron/refresh-competitors` with schedule `0 6 * * *`.
  </verify>
  <done>
`vercel.json` exists at project root with daily cron schedule for the refresh-competitors endpoint at 6:00 AM UTC.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors in new files
2. `verifyCronAuth` returns `null` on valid auth and `NextResponse` with 401 on invalid
3. Cron route exports `GET` and `maxDuration`
4. Cron route uses `createServiceClient()` (not auth client -- no user session in cron)
5. Cron route uses `createScrapingProvider()` (not direct ApifyClient import)
6. Each handle's scrape is wrapped in individual try/catch (failure isolation)
7. Failed handles get `scrape_status: "failed"` and the loop continues
8. Snapshot upsert uses `onConflict: "competitor_id,snapshot_date"`
9. Route calls `revalidatePath("/competitors")` after batch processing
10. `vercel.json` is valid JSON with correct cron path and schedule
</verification>

<success_criteria>
- Cron auth utility provides reusable CRON_SECRET verification
- Cron route re-scrapes all tracked competitor profiles with fresh data
- Daily snapshots are upserted (one per profile per day) for time-series
- Individual handle failures are isolated and don't block the batch
- vercel.json configures daily trigger at 6:00 AM UTC
</success_criteria>

<output>
After completion, create `.planning/phases/02-competitor-management/02-02-SUMMARY.md`
</output>
