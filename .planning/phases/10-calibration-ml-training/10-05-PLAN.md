---
phase: 10-calibration-ml-training
plan: 05
type: execute
wave: 3
depends_on: ["10-01", "10-02"]
files_modified:
  - src/app/api/cron/calibration-audit/route.ts
  - src/app/api/cron/retrain-ml/route.ts
autonomous: true

must_haves:
  truths:
    - "Monthly calibration audit cron computes ECE and re-fits Platt parameters"
    - "Cron alerts (logs warning) when ECE exceeds 0.15 drift threshold"
    - "retrain-ml cron is updated to trigger ML model retraining from training data"
    - "Both crons follow the verifyCronAuth pattern and return structured JSON responses"
  artifacts:
    - path: "src/app/api/cron/calibration-audit/route.ts"
      provides: "Monthly calibration audit cron endpoint"
      exports: ["GET"]
    - path: "src/app/api/cron/retrain-ml/route.ts"
      provides: "Updated ML retraining cron endpoint"
      exports: ["GET"]
  key_links:
    - from: "src/app/api/cron/calibration-audit/route.ts"
      to: "src/lib/engine/calibration.ts"
      via: "imports generateCalibrationReport and fitPlattScaling"
      pattern: "import.*from.*calibration"
    - from: "src/app/api/cron/retrain-ml/route.ts"
      to: "src/lib/engine/ml.ts"
      via: "imports trainModel"
      pattern: "import.*from.*ml"
---

<objective>
Create the monthly calibration audit cron that measures ECE, re-fits Platt scaling parameters, and alerts on calibration drift. Also update the existing retrain-ml cron stub to trigger actual ML model retraining.

Purpose: Without periodic re-calibration, prediction accuracy degrades as content trends shift. The audit cron ensures the system self-corrects monthly by re-fitting calibration parameters and flagging when drift exceeds acceptable thresholds.

Output: New `calibration-audit` cron route and updated `retrain-ml` cron route.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/app/api/cron/validate-rules/route.ts
@src/app/api/cron/retrain-ml/route.ts
@src/lib/cron-auth.ts
@.planning/phases/10-calibration-ml-training/10-01-SUMMARY.md
@.planning/phases/10-calibration-ml-training/10-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration-audit cron route</name>
  <files>src/app/api/cron/calibration-audit/route.ts</files>
  <action>
Create `src/app/api/cron/calibration-audit/route.ts` following the established cron pattern from `validate-rules/route.ts`:

**Handler: `GET`**

1. Auth: `verifyCronAuth(request)` from `@/lib/cron-auth`
2. Generate calibration report: `generateCalibrationReport({ sinceDays: 90 })` — 90-day window for meaningful sample
3. If totalSamples < 50: return `{ status: "skipped", reason: "Insufficient outcome data", samples: totalSamples, required: 50 }`
4. Check ECE drift: if `report.ece > 0.15`:
   - Log warning: `[calibration-audit] DRIFT ALERT: ECE ${report.ece} exceeds threshold 0.15`
   - Set `driftDetected = true`
5. Re-fit Platt scaling:
   - Call `fetchOutcomePairs()` from calibration.ts
   - Call `fitPlattScaling(pairs)` — this produces fresh A, B coefficients
   - If fit successful (non-null), log new parameters
   - If fit returns null (<50 pairs), log "Not enough data for Platt fit"
6. Invalidate Platt parameter cache so next request uses fresh params (call the cache's invalidate method — the cache is in calibration.ts, export an `invalidatePlattCache()` function)
7. Return structured JSON response:

```json
{
  "status": "completed",
  "ece": 0.12,
  "driftDetected": false,
  "driftThreshold": 0.15,
  "bins": [...],
  "totalSamples": 150,
  "plattRefitted": true,
  "plattParams": { "a": -1.2, "b": 0.3 },
  "auditedAt": "2026-02-16T..."
}
```

8. Wrap in try/catch, log errors with `[calibration-audit]` prefix, return 500 on failure.

**Important:**
- This is a monthly cron (configured in vercel.json) — not high-frequency
- The 90-day lookback window ensures enough data for meaningful ECE
- Drift threshold of 0.15 ECE is standard — below 0.10 is well-calibrated, 0.10-0.15 is acceptable, above 0.15 needs attention
- Add `invalidatePlattCache()` export to calibration.ts if not already present — simple function that calls cache.invalidate("platt-params")
  </action>
  <verify>
Run `npx tsc --noEmit src/app/api/cron/calibration-audit/route.ts` to verify compilation.
Verify it exports a `GET` function.
  </verify>
  <done>
Calibration audit cron exists, computes ECE, re-fits Platt parameters, logs drift alert when ECE > 0.15. Returns structured JSON with all calibration metrics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update retrain-ml cron to trigger actual ML training</name>
  <files>src/app/api/cron/retrain-ml/route.ts</files>
  <action>
Replace the existing stub implementation in `src/app/api/cron/retrain-ml/route.ts` with actual ML retraining:

**Updated `GET` handler:**

1. Auth: `verifyCronAuth(request)` (keep existing)
2. Keep the outcome count check but lower threshold to 100 (was 1000 — we have 7000+ scraped videos for training data, outcomes are supplementary)
3. Import `trainModel` from `@/lib/engine/ml`
4. Try to train: `const result = await trainModel()`
5. On success, return:
```json
{
  "status": "completed",
  "trainAccuracy": 0.45,
  "testAccuracy": 0.42,
  "confusionMatrix": [[...], ...],
  "trainedAt": "2026-02-16T..."
}
```
6. On error during training, log and return:
```json
{
  "status": "failed",
  "error": "Training failed: {message}"
}
```
7. Keep the outcome count in the response for monitoring but don't gate training on it (training uses scraped_videos via training-data.json, not outcomes)

**Changes from stub:**
- Remove `MIN_OUTCOMES_FOR_TRAINING = 1000` — training data comes from scraped videos not outcomes
- Add `trainModel()` import and call
- Return actual training metrics instead of "pending_implementation"
- Keep error handling pattern, update log prefix to `[retrain-ml]`

**Important:**
- Training may take 5-15 seconds for 200 epochs on 7000 samples — this is fine for a weekly cron
- The Vercel function timeout (default 10s on Hobby, 60s on Pro) may be a concern — add a comment noting Pro plan needed for training
- Training writes ml-weights.json to disk — on Vercel this is ephemeral (cleared on redeploy). For production, weights should be stored in Supabase Storage or env var. Add a TODO comment for this.
  </action>
  <verify>
Run `npx tsc --noEmit src/app/api/cron/retrain-ml/route.ts` to verify compilation.
Verify it imports from `@/lib/engine/ml`.
  </verify>
  <done>
retrain-ml cron triggers actual ML training, returns accuracy metrics and confusion matrix. Replaces the previous stub with real model training pipeline. Training data sourced from scraped videos via training-data.json.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes for both cron routes
- calibration-audit imports from calibration.ts
- retrain-ml imports from ml.ts
- Both follow verifyCronAuth pattern
</verification>

<success_criteria>
- Monthly calibration audit cron re-fits parameters and alerts on drift (ECE > 0.15)
- retrain-ml cron triggers real ML training with accuracy reporting
- Both crons return structured JSON with metrics for monitoring
</success_criteria>

<output>
After completion, create `.planning/phases/10-calibration-ml-training/10-05-SUMMARY.md`
</output>
