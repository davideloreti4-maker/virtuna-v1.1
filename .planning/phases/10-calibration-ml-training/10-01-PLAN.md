---
phase: 10-calibration-ml-training
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/engine/calibration.ts
autonomous: true

must_haves:
  truths:
    - "ECE computation produces a single numeric error value (0-1) from binned predicted vs actual scores"
    - "Calibration report includes per-bin accuracy, overall ECE, and sample count"
    - "Function handles edge cases: empty outcomes, single bin, all-same predictions"
  artifacts:
    - path: "src/lib/engine/calibration.ts"
      provides: "ECE measurement pipeline with computeECE and generateCalibrationReport"
      exports: ["computeECE", "generateCalibrationReport", "CalibrationReport", "CalibrationBin"]
  key_links:
    - from: "src/lib/engine/calibration.ts"
      to: "outcomes table"
      via: "createServiceClient query"
      pattern: "supabase\\.from\\(\"outcomes\"\\)"
---

<objective>
Build the Expected Calibration Error (ECE) measurement pipeline that computes prediction accuracy by comparing predicted scores against actual outcomes from the outcomes table.

Purpose: ECE is the foundation for all calibration work — Platt scaling (Plan 10-02) and the monthly audit cron (Plan 10-05) both depend on being able to measure calibration error. Without this, we cannot know if the engine is well-calibrated.

Output: `src/lib/engine/calibration.ts` with `computeECE()`, `generateCalibrationReport()`, and supporting types.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/lib/engine/types.ts
@src/lib/engine/aggregator.ts
@src/lib/supabase/service.ts
@src/app/api/cron/validate-rules/route.ts
@supabase/migrations/20260213000000_content_intelligence.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration.ts with ECE computation and calibration report</name>
  <files>src/lib/engine/calibration.ts</files>
  <action>
Create `src/lib/engine/calibration.ts` with the following:

**Types:**
- `CalibrationBin`: `{ binStart: number; binEnd: number; avgPredicted: number; avgActual: number; count: number; gap: number }`
- `CalibrationReport`: `{ ece: number; bins: CalibrationBin[]; totalSamples: number; generatedAt: string }`
- `OutcomePair`: `{ predicted: number; actual: number }` (internal)

**Functions:**

1. `fetchOutcomePairs(supabase, options?: { sinceDays?: number })` — async function
   - Query `outcomes` table joining `analysis_results` for predicted_score (use `overall_score` from analysis_results) and actual_score from outcomes
   - Filter: `deleted_at IS NULL`, `actual_score IS NOT NULL`, `predicted_score IS NOT NULL`
   - If `sinceDays` provided, filter `outcomes.created_at >= now - sinceDays`
   - Use the same cast-through-unknown pattern from validate-rules cron for Supabase type assertions
   - Return `OutcomePair[]` with predicted and actual normalized to 0-1 range (divide by 100, since scores are 0-100)

2. `computeECE(pairs: OutcomePair[], numBins = 10)` — pure function
   - Implements Expected Calibration Error: ECE = sum(|bin_count/total| * |avg_predicted - avg_actual|) for each bin
   - Bin predicted scores into `numBins` equal-width bins from 0 to 1
   - For each bin: compute avg predicted, avg actual, count, gap = |avg_predicted - avg_actual|
   - Return `{ ece, bins }` where `ece` is the weighted average gap
   - Handle edge cases: empty pairs returns ECE=0 with empty bins, bins with 0 samples get gap=0

3. `generateCalibrationReport(options?: { sinceDays?: number })` — async function
   - Creates Supabase service client
   - Calls `fetchOutcomePairs()` then `computeECE()`
   - Returns `CalibrationReport` with `ece`, `bins`, `totalSamples`, `generatedAt`
   - If no outcome pairs found, returns report with ECE=0, empty bins, totalSamples=0

Export all public types and functions. Keep `fetchOutcomePairs` exported for reuse by Plan 10-02 (Platt scaling) and Plan 10-05 (audit cron).

**Important implementation details:**
- Scores in the DB are 0-100 (overall_score, predicted_score, actual_score). Normalize to 0-1 for ECE computation.
- Use `createServiceClient()` from `@/lib/supabase/service` (same pattern as validate-rules cron)
- Do NOT use `analysis_results.rule_contributions` — that's for rule accuracy, not calibration
- The outcomes table has `predicted_score` and `actual_score` columns directly
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/engine/calibration.ts` to verify TypeScript compilation.
Verify exports: `computeECE`, `generateCalibrationReport`, `fetchOutcomePairs`, `CalibrationReport`, `CalibrationBin` are all exported.
  </verify>
  <done>
calibration.ts exists with working ECE computation. computeECE([{predicted:0.8,actual:0.7},{predicted:0.3,actual:0.2}], 10) produces a numeric ECE value between 0 and 1. generateCalibrationReport() queries the outcomes table and returns a CalibrationReport.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create admin calibration-report API route</name>
  <files>src/app/api/admin/calibration-report/route.ts</files>
  <action>
Create `src/app/api/admin/calibration-report/route.ts`:

- `GET` handler protected by `verifyCronAuth(request)` from `@/lib/cron-auth` (reuse existing auth pattern — admin routes use same Bearer token auth as crons)
- Optional query param: `?days=30` to filter outcomes window (default: all time)
- Parse `days` from URL searchParams, pass as `sinceDays` to `generateCalibrationReport()`
- Return JSON: the full `CalibrationReport` object
- Wrap in try/catch, log errors to console with `[calibration-report]` prefix, return 500 on failure
- Follow the exact same pattern as `src/app/api/cron/validate-rules/route.ts` for structure

**Response shape:**
```json
{
  "ece": 0.12,
  "bins": [{ "binStart": 0, "binEnd": 0.1, "avgPredicted": 0.05, "avgActual": 0.03, "count": 15, "gap": 0.02 }, ...],
  "totalSamples": 150,
  "generatedAt": "2026-02-16T..."
}
```
  </action>
  <verify>
Run `npx tsc --noEmit src/app/api/admin/calibration-report/route.ts` to verify compilation.
Verify the route exports a `GET` function.
  </verify>
  <done>
Admin calibration report endpoint exists at /api/admin/calibration-report, returns CalibrationReport JSON with ECE value and per-bin breakdown. Protected by CRON_SECRET Bearer auth.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes for both new files
- `computeECE` is a pure function that can be tested with known inputs
- Admin route follows established cron auth pattern
</verification>

<success_criteria>
- ECE measurement pipeline computes calibration error from predicted vs actual scores
- Per-bin accuracy breakdown available via admin API
- All types exported for downstream plans (10-02, 10-05)
</success_criteria>

<output>
After completion, create `.planning/phases/10-calibration-ml-training/10-01-SUMMARY.md`
</output>
