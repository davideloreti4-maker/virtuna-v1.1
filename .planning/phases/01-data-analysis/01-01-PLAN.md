---
phase: 01-data-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/analyze-dataset.ts
  - src/lib/engine/calibration-baseline.json
  - .planning/phases/01-data-analysis/data-analysis-report.md
autonomous: true
user_setup:
  - service: supabase
    why: "Script uses service role key to query scraped_videos table (bypasses RLS)"
    env_vars:
      - name: NEXT_PUBLIC_SUPABASE_URL
        source: "Already configured in .env.local"
      - name: SUPABASE_SERVICE_ROLE_KEY
        source: "Already configured in .env.local"

must_haves:
  truths:
    - "Script queries scraped_videos table and deduplicates by platform_video_id"
    - "Outlier filtering removes celebrity accounts (10M+ followers proxy) and 0-view videos"
    - "Engagement rate normalized by account reach is computed as the primary virality metric"
    - "5 virality tiers defined with data-driven thresholds (0-25, 25-45, 45-65, 65-80, 80-100)"
    - "Key differentiators between viral-tier and average-tier content are computed"
    - "calibration-baseline.json contains machine-readable patterns and thresholds"
    - "Markdown summary report contains human-readable analysis with data quality section"
  artifacts:
    - path: "scripts/analyze-dataset.ts"
      provides: "Data analysis script"
      min_lines: 200
    - path: "src/lib/engine/calibration-baseline.json"
      provides: "Machine-readable virality patterns and thresholds"
      contains: "virality_tiers"
    - path: ".planning/phases/01-data-analysis/data-analysis-report.md"
      provides: "Human-readable analysis summary"
      contains: "Data Quality"
  key_links:
    - from: "scripts/analyze-dataset.ts"
      to: "src/lib/supabase/service.ts"
      via: "createServiceClient import"
      pattern: "createServiceClient"
    - from: "scripts/analyze-dataset.ts"
      to: "src/lib/engine/calibration-baseline.json"
      via: "fs.writeFileSync output"
      pattern: "calibration-baseline\\.json"
---

<objective>
Build and run a data analysis script that mines ~5000 scraped TikTok videos from Supabase for virality patterns and thresholds. Produces two outputs: `calibration-baseline.json` (machine-readable patterns for downstream engine phases) and a markdown summary report (human-readable insights).

Purpose: Downstream phases (Gemini prompts, DeepSeek CoT, aggregation formula, ML training, calibration) all depend on real data patterns. Without this analysis, prompts would use guesses instead of evidence.

Output:
- `src/lib/engine/calibration-baseline.json` — virality tiers, engagement thresholds, duration sweet spots, top patterns
- `.planning/phases/01-data-analysis/data-analysis-report.md` — human-readable summary with data quality section
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/lib/supabase/service.ts
@supabase/migrations/20260213000000_content_intelligence.sql
@src/app/api/webhooks/apify/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build scraped video data analysis script</name>
  <files>scripts/analyze-dataset.ts</files>
  <action>
Create `scripts/analyze-dataset.ts` — a TypeScript script that runs via `npx tsx scripts/analyze-dataset.ts`.

**Environment setup:**
- Load env vars from `.env.local` using `dotenv` or inline `process.env` reads (the script runs via tsx which loads .env automatically if configured; if not, add `import 'dotenv/config'` at top and use `dotenv` as a dev dependency — check if already in package.json first, install with `pnpm add -D dotenv` if needed)
- Create Supabase client using the same pattern as `src/lib/supabase/service.ts` but with direct `createClient` from `@supabase/supabase-js` (not the SSR variant, since this is a standalone script, not a Next.js server component). Import `Database` type from `src/types/database.types.ts`.

**Step 1: Fetch and Deduplicate**
- Query `scraped_videos` table using service role client (bypasses RLS). Fetch ALL non-archived rows: `select('*').is('archived_at', null)`. Paginate if needed (Supabase default limit is 1000 rows, use `.range()` to fetch in batches of 1000).
- Deduplicate by `platform_video_id` — keep the latest row (highest `created_at`) if duplicates exist.
- Log: total fetched, duplicates removed, unique count.

**Step 2: Outlier Filtering**
- Remove videos with `views === 0` or `views === null` (no engagement data).
- Remove probable celebrity/mega-influencer content: since `scraped_videos` has no follower count, use a views-based proxy — remove videos with views > p99.5 of the dataset (extreme outlier threshold). This catches celebrity content without a follower field.
- Remove videos with null/0 `duration_seconds` (unusable for duration analysis).
- Track all exclusion counts and reasons for the data quality section.

**Step 3: Compute Engagement Metrics**
- For each video, compute:
  - `engagement_rate = (likes + comments + shares) / views` (basic engagement rate)
  - `like_ratio = likes / views`
  - `comment_ratio = comments / views`
  - `share_ratio = shares / views`
  - `likes_per_view = likes / views` (same as like_ratio, keep naming consistent)
- Note: We don't have follower counts in scraped_videos, so "normalized by account size" means normalized by reach (views). The engagement rate relative to views IS the normalization — a 10K view video with 2K likes (20% ER) is more viral-signal than a 10M view video with 200K likes (2% ER).

**Step 4: Statistical Analysis**
- Compute for the entire filtered dataset:
  - Distribution percentiles (p10, p25, p50/median, p75, p90, p95, p99) for: views, likes, shares, comments, engagement_rate, duration_seconds
  - Mean and standard deviation for each metric
  - Count videos per `category` field (if populated)

**Step 5: Derive Virality Tiers**
- Use engagement_rate distribution to define 5 tiers with data-driven boundaries:
  - Tier 1 (0-25 "Unlikely to perform"): engagement_rate below p25
  - Tier 2 (25-45 "Below average"): engagement_rate p25 to p50
  - Tier 3 (45-65 "Average"): engagement_rate p50 to p75
  - Tier 4 (65-80 "Strong potential"): engagement_rate p75 to p90
  - Tier 5 (80-100 "Viral potential"): engagement_rate above p90
- Map these percentile-based ER boundaries to the 0-100 score range. Store the actual ER thresholds that correspond to each tier boundary.
- Validate distribution: most videos should fall in the 40-65 range (tiers 2-3). If the distribution is heavily skewed, note it and adjust mapping.

**Step 6: Compute Key Differentiators (Viral vs Average)**
- Split dataset into viral-tier (top 10%, p90+) and average-tier (p40-p60).
- Compare across ALL fields:
  - Duration: mean duration of viral vs average. Compute "viral videos are X% shorter/longer".
  - Hashtag count: mean hashtag count viral vs average.
  - Caption length: mean `description.length` viral vs average (light analysis per user decision).
  - Sound usage: what % of viral videos have a `sound_name` vs average.
  - Share ratio: mean share_ratio viral vs average (shares are the strongest virality signal on TikTok).
  - Comment ratio: mean comment_ratio viral vs average.
- Express each as a human-readable differentiator string, e.g., "Viral videos are 23% shorter on average (18s vs 23s)".

**Step 7: Pattern Mining**
- **Duration sweet spot**: Find the duration range with the highest median engagement_rate. Bucket durations into 5s intervals (0-5s, 5-10s, ..., 55-60s, 60s+). Find the bucket with highest median ER.
- **Top hashtags**: Count frequency of each hashtag across all videos. Get top 50 by frequency. For top 20, also compute their median engagement_rate. Flag hashtags that are high-frequency AND high-ER as "power hashtags".
- **Top sounds**: Count frequency of each `sound_name`. Get top 30 by frequency. For top 15, compute median ER. Flag sounds that appear in viral-tier videos disproportionately.
- **Engagement ratio patterns**: Compute the typical like:comment:share ratio. E.g., "For every 100 likes, there are ~X comments and ~Y shares".
- **Category breakdown**: If `category` field has data, compute per-category stats (count, median ER, median views).

**Step 8: Write calibration-baseline.json**
- Write to `src/lib/engine/calibration-baseline.json` with this structure (Claude's discretion on exact shape, but must include):

```json
{
  "generated_at": "ISO timestamp",
  "dataset_stats": {
    "total_fetched": number,
    "duplicates_removed": number,
    "outliers_removed": number,
    "analyzed_count": number
  },
  "virality_tiers": [
    {
      "tier": 1,
      "label": "Unlikely to perform",
      "score_range": [0, 25],
      "engagement_rate_threshold": { "min": number, "max": number },
      "video_count": number,
      "percentage": number
    }
  ],
  "engagement_percentiles": {
    "p10": number, "p25": number, "p50": number, "p75": number, "p90": number, "p95": number, "p99": number
  },
  "duration_sweet_spot": {
    "optimal_range_seconds": [number, number],
    "median_engagement_rate": number,
    "duration_buckets": [{ "range": string, "count": number, "median_er": number }]
  },
  "top_hashtags": [{ "tag": string, "count": number, "median_er": number, "is_power_hashtag": boolean }],
  "top_sounds": [{ "name": string, "count": number, "median_er": number, "viral_overrepresentation": number }],
  "engagement_ratios": {
    "likes_per_100_views": number,
    "comments_per_100_views": number,
    "shares_per_100_views": number,
    "like_comment_share_ratio": string
  },
  "key_differentiators": [
    { "factor": string, "viral_avg": number, "average_avg": number, "difference_pct": number, "description": string }
  ],
  "view_percentiles": {
    "p25": number, "p50": number, "p75": number, "p90": number, "p99": number
  },
  "categories": [{ "name": string, "count": number, "median_er": number, "median_views": number }]
}
```

**Step 9: Write markdown summary report**
- Write to `.planning/phases/01-data-analysis/data-analysis-report.md`
- Sections:
  1. **Executive Summary** — 3-5 bullet points of key findings
  2. **Data Quality** — total records, duplicates removed, outliers filtered (with counts and reasons), final analyzed count
  3. **Virality Tiers** — table showing tier boundaries, ER thresholds, video counts, percentages
  4. **Key Differentiators** — what separates viral from average (the human-readable strings)
  5. **Duration Analysis** — sweet spot, bucket breakdown
  6. **Hashtag Analysis** — top hashtags, power hashtags
  7. **Sound Analysis** — top sounds, viral-overrepresented sounds
  8. **Engagement Patterns** — ratio breakdowns
  9. **Category Breakdown** — if data exists
  10. **Implications for Engine** — 3-5 bullet points on how downstream phases should use this data

**Code quality:**
- Use TypeScript strict mode. No `any` types (use proper types for Supabase rows).
- Add inline comments explaining statistical choices.
- Handle empty dataset gracefully (log warning, exit with code 0, produce empty-state JSON).
- Use `console.log` with `[analyze]` prefix for all logging.
- Make the script idempotent — running it twice overwrites previous output.
  </action>
  <verify>
Run `npx tsx scripts/analyze-dataset.ts` from the project root. Script should:
1. Connect to Supabase and fetch scraped_videos
2. Log deduplication and filtering stats
3. Output `src/lib/engine/calibration-baseline.json` (valid JSON, parseable with `JSON.parse`)
4. Output `.planning/phases/01-data-analysis/data-analysis-report.md`
5. Exit with code 0

Verify JSON structure: `node -e "const d = require('./src/lib/engine/calibration-baseline.json'); console.log(Object.keys(d)); console.log('Tiers:', d.virality_tiers?.length); console.log('Analyzed:', d.dataset_stats?.analyzed_count)"`
  </verify>
  <done>
- `scripts/analyze-dataset.ts` exists and runs without errors against live Supabase data
- `src/lib/engine/calibration-baseline.json` contains valid JSON with virality_tiers, engagement_percentiles, duration_sweet_spot, top_hashtags, top_sounds, key_differentiators
- `.planning/phases/01-data-analysis/data-analysis-report.md` exists with all 10 sections including Data Quality
- Dataset stats show deduplication removed ~40% of records (per user estimate)
- 5 virality tiers defined with engagement_rate thresholds derived from real data distribution
- Key differentiators show measurable differences between viral and average content
  </done>
</task>

<task type="auto">
  <name>Task 2: Validate outputs and sanity-check data patterns</name>
  <files>src/lib/engine/calibration-baseline.json, .planning/phases/01-data-analysis/data-analysis-report.md</files>
  <action>
After Task 1 produces the outputs, perform validation:

1. **JSON schema validation**: Read `calibration-baseline.json` and verify:
   - `dataset_stats.analyzed_count` > 0 (script actually processed data)
   - `virality_tiers` has exactly 5 entries
   - All tier `engagement_rate_threshold` values are monotonically increasing
   - `engagement_percentiles` has all required keys (p10 through p99)
   - `duration_sweet_spot.optimal_range_seconds` is a 2-element array with reasonable values (e.g., 5-60s)
   - `top_hashtags` has at least 10 entries
   - `top_sounds` has at least 5 entries
   - `key_differentiators` has at least 3 entries

2. **Sanity checks on values**:
   - Engagement rate percentiles should be reasonable (p50 between 0.01 and 0.30 — i.e., 1-30% engagement rate). If outside this range, investigate and document why.
   - Duration sweet spot should be between 7-60 seconds (TikTok norms). If outside, note it.
   - View percentiles: p50 should be in thousands to tens-of-thousands range for FYP-scraped content.
   - Tier distribution: verify tier 3 (Average) contains the most videos (~25-30% of dataset).

3. **Report completeness**: Read the markdown report and verify all 10 sections are present and contain actual data (not placeholders).

4. **Fix any issues**: If validation reveals problems (e.g., a threshold is NaN, a section is empty, counts don't add up), fix the analysis script and re-run.

5. **Add a script entry to package.json** for convenience: add `"analyze": "npx tsx scripts/analyze-dataset.ts"` to the scripts section so it can be run via `pnpm analyze` in the future.
  </action>
  <verify>
Run validation checks:
- `node -e "const d = require('./src/lib/engine/calibration-baseline.json'); console.log('Tiers:', d.virality_tiers.length); console.log('Analyzed:', d.dataset_stats.analyzed_count); console.log('P50 ER:', d.engagement_percentiles.p50); console.log('Sweet spot:', d.duration_sweet_spot.optimal_range_seconds); console.log('Hashtags:', d.top_hashtags.length); console.log('Differentiators:', d.key_differentiators.length);"`
- All values are reasonable numbers (no NaN, no Infinity, no null)
- `pnpm analyze` runs successfully (package.json script added)
  </verify>
  <done>
- calibration-baseline.json passes all schema and sanity checks
- All engagement rate values are within reasonable bounds for TikTok content
- 5 virality tiers have monotonically increasing thresholds
- Duration sweet spot is a plausible TikTok duration range
- Markdown report has all 10 sections with real data
- package.json has `analyze` script entry
- Output is ready for consumption by downstream phases (Phase 2: Gemini prompts, Phase 3: DeepSeek CoT, Phase 10: ML training)
  </done>
</task>

</tasks>

<verification>
1. `npx tsx scripts/analyze-dataset.ts` completes with exit code 0
2. `src/lib/engine/calibration-baseline.json` is valid JSON with all required sections
3. `.planning/phases/01-data-analysis/data-analysis-report.md` has all 10 sections
4. Virality tiers are data-driven (thresholds from actual engagement rate distribution, not hardcoded)
5. Key differentiators provide measurable differences (e.g., "viral videos are X% shorter")
6. Data quality section shows deduplication and outlier filtering with counts
</verification>

<success_criteria>
- CAL-01 requirement satisfied: data analysis script mines scraped videos for virality patterns and thresholds
- Phase 1 success criteria met:
  1. Script runs against scraped_videos table and outputs calibration-baseline.json
  2. Virality thresholds defined from real data distribution
  3. Duration sweet spot, top hashtags, top sounds, and engagement ratios documented
- Outputs ready for Phase 2 (Gemini prompt data), Phase 3 (DeepSeek CoT data), Phase 10 (ML training data)
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-analysis/01-01-SUMMARY.md`
</output>
