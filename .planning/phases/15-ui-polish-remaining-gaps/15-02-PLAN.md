---
phase: 15-ui-polish-remaining-gaps
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark.ts
autonomous: true

must_haves:
  truths:
    - "Benchmark script has clear env-gated documentation explaining required API keys and how to run"
    - "Running the benchmark with missing keys produces a clear warning message (not a crash)"
    - "Benchmark script compiles with TypeScript"
  artifacts:
    - path: "scripts/benchmark.ts"
      provides: "Production-ready benchmark script with env documentation"
      contains: "DEEPSEEK_API_KEY"
  key_links:
    - from: "scripts/benchmark.ts"
      to: "src/lib/engine/pipeline"
      via: "import runPredictionPipeline"
      pattern: "runPredictionPipeline"
---

<objective>
Validate the benchmark script compiles and document it as env-gated with clear setup instructions.

Purpose: Close the benchmark validation success criterion. The script already handles missing API keys gracefully (warns and continues). This plan adds clear documentation header and verifies the script compiles correctly via TypeScript type-checking.

Output: Documented benchmark script verified to compile.
</objective>

<execution_context>
@/Users/davideloreti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davideloreti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@scripts/benchmark.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add env-gated documentation and verify compilation</name>
  <files>scripts/benchmark.ts</files>
  <action>
Add a documentation block at the top of `scripts/benchmark.ts` (after the dotenv import, before the dynamic imports section) with the following:

```
/**
 * Virtuna v2 Accuracy Benchmark
 *
 * Runs 50 diverse content samples through the full prediction pipeline and
 * produces benchmark-results.json with score distributions, latency, cost,
 * and completeness metrics.
 *
 * Prerequisites:
 *   Copy .env.local.example to .env.local and populate:
 *   - NEXT_PUBLIC_SUPABASE_URL — Supabase project URL
 *   - SUPABASE_SERVICE_ROLE_KEY — Supabase service role key
 *   - GEMINI_API_KEY — Google AI Gemini API key
 *   - DEEPSEEK_API_KEY — DeepSeek API key
 *
 * Usage:
 *   npx tsx scripts/benchmark.ts
 *
 * Output:
 *   scripts/benchmark-results.json (gitignored)
 *
 * Note: Missing API keys will cause individual samples to fail with error
 * messages but the script will continue and produce a results file with
 * partial data. All 4 keys are required for meaningful benchmark results.
 */
```

Then verify the script compiles:
- Run `npx tsc --noEmit scripts/benchmark.ts` or `pnpm build` to confirm no type errors.
- If tsc flags errors due to path aliases (tsconfig-paths is runtime only), verify that `pnpm build` succeeds instead (the benchmark is run with tsx, not compiled).

Do NOT change any logic or behavior in the script. Documentation only.
  </action>
  <verify>
    `pnpm build` passes.
    `head -30 scripts/benchmark.ts` shows the documentation block.
    `grep "Prerequisites" scripts/benchmark.ts` confirms documentation exists.
  </verify>
  <done>Benchmark script has clear env-gated documentation at the top explaining required API keys, usage command, and output location. Script compiles without errors.</done>
</task>

</tasks>

<verification>
1. `pnpm build` passes
2. `scripts/benchmark.ts` has documentation header with Prerequisites, Usage, Output sections
3. Missing API keys produce warnings, not crashes (already implemented — verify with grep)
</verification>

<success_criteria>
- Benchmark script has documentation header with all 4 required env vars listed
- Script compiles without TypeScript errors
- Missing key handling preserved (warn and continue pattern)
</success_criteria>

<output>
After completion, create `.planning/phases/15-ui-polish-remaining-gaps/15-02-SUMMARY.md`
</output>
